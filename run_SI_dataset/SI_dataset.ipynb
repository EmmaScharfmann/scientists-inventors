{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3678b3b0-4dcd-4732-9e13-6ca1e504d68d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Import packages and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aad52041-9274-439b-8d8a-8225c0966ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##import packages \n",
    "\n",
    "import pandas as pd\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "import random \n",
    "import numpy as np\n",
    "import json, requests \n",
    "import geopy.distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import unicodedata\n",
    "from metaphone import doublemetaphone\n",
    "from fuzzywuzzy import fuzz\n",
    "from difflib import SequenceMatcher\n",
    "import re\n",
    "import pickle5 as pickle\n",
    "from tqdm import tqdm\n",
    "import psycopg2\n",
    "import spacy\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "import glob \n",
    "import networkx as nx\n",
    "from tqdm import tqdm \n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "from mpi4py import MPI\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#!python -m spacy download en_core_web_lg\n",
    "#spacy_nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d404c928-e4a6-4f5a-99d6-948ef040edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = '/home/fs01/spec1142/Emma/GateKeepers/'\n",
    "\n",
    "f = open(main_path + \"database.txt\", \"r\")\n",
    "user , password = f.read().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf6f30-048c-4df1-bae5-e3ecd0b7d059",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Create first name and last name frequency dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa300e4c-c87a-40b1-b271-b117d5511254",
   "metadata": {},
   "outputs": [],
   "source": [
    "## frequency of the first names \n",
    "\n",
    "#establishing the connection\n",
    "conn = psycopg2.connect(\"user=\" + user + \" password=\" + password)\n",
    "\n",
    "#Creating a cursor object using the cursor() method\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "#Creating table as per requirement\n",
    "sql =\"\"\"SELECT COUNT(*), disambig_inventor_name_first\n",
    "        FROM  inventors_PatentsView \n",
    "        GROUP BY disambig_inventor_name_first;\"\"\"\n",
    "\n",
    "cursor.execute(sql)\n",
    "result = cursor.fetchall()\n",
    "\n",
    "\n",
    "#Closing the connection\n",
    "conn.close()\n",
    "\n",
    "\n",
    "dic_first_names = {} \n",
    "\n",
    "## count the number of occurences of each name in PV\n",
    "for elem in tqdm(result):\n",
    "    value , name = elem \n",
    "    if name != None:\n",
    "        norm_name = normalize_names(name)\n",
    "        if norm_name != '':\n",
    "            if norm_name not in dic_first_names:\n",
    "                dic_first_names[norm_name] = 0 \n",
    "            dic_first_names[norm_name] += value\n",
    "            \n",
    "            \n",
    "            \n",
    "dic_first_names = {k: v for k, v in sorted(dic_first_names.items(), key=lambda item: item[1] , reverse = True)}            \n",
    "\n",
    "\n",
    "## normalize the frequency of the names \n",
    "data = list(dic_first_names.values())\n",
    "v_max = np.log(1+np.max(data))\n",
    "v_min = np.log(1+np.min(data))\n",
    "\n",
    "for elem in dic_first_names:\n",
    "    dic_first_names[elem] = ( np.log(1 + dic_first_names[elem]) - v_min )/ (v_max - v_min)\n",
    "\n",
    "\n",
    "import json\n",
    "json = json.dumps(dic_first_names)\n",
    "f = open(main_path + \"frequency_first_names.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb8b8f-1b85-45d0-8f8c-d12e73ee7310",
   "metadata": {},
   "outputs": [],
   "source": [
    "## frequency of the last names \n",
    "\n",
    "#establishing the connection\n",
    "conn = psycopg2.connect(\"user=\" + user + \" password=\" + password)\n",
    "\n",
    "#Creating a cursor object using the cursor() method\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "#Creating table as per requirement\n",
    "sql =\"\"\"SELECT COUNT(*), disambig_inventor_name_last\n",
    "        FROM  inventors_PatentsView \n",
    "        GROUP BY disambig_inventor_name_last;\"\"\"\n",
    "\n",
    "cursor.execute(sql)\n",
    "result = cursor.fetchall()\n",
    "\n",
    "\n",
    "#Closing the connection\n",
    "conn.close()\n",
    "\n",
    "\n",
    "dic_last_names = {} \n",
    "\n",
    "## count the number of occurences of each name in PV\n",
    "for elem in tqdm(result):\n",
    "    value , name = elem \n",
    "    if name != None:\n",
    "        \n",
    "        norm_name = normalize_names(name)\n",
    "        norm_name = \" \".join([ elem for elem in norm_name.split(\" \") if len(elem) > 1])\n",
    "        \n",
    "        if norm_name != '':\n",
    "            if norm_name not in dic_last_names:\n",
    "                dic_last_names[norm_name] = 0 \n",
    "            dic_last_names[norm_name] += value\n",
    "            \n",
    "            \n",
    "dic_last_names = {k: v for k, v in sorted(dic_last_names.items(), key=lambda item: item[1] , reverse = True)}\n",
    "\n",
    "\n",
    "## normalize the frequency of the names \n",
    "data = list(dic_last_names.values())\n",
    "v_max = np.log(np.max(data))\n",
    "v_min = np.log(np.min(data))\n",
    "\n",
    "for elem in dic_last_names:\n",
    "    dic_last_names[elem] = ( np.log(1 + dic_last_names[elem]) - v_min )/ (v_max - v_min)\n",
    "\n",
    "\n",
    "import json\n",
    "json = json.dumps(dic_last_names)\n",
    "f = open(main_path + \"frequency_last_names.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c242a6c6-9659-4afc-bd45-4d86e1cd8551",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Identifying SIs from a last name - try yourself on a given last name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b109dd4-bf33-4c3c-a258-1e061038a9c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38f7d67d-4204-4946-9bd5-911eb2ddd030",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell is similar to the section \"Files to import or to modify\"  of the codes \"gatekeepers_uncommon_names.py\" and \"gatekeepers_uncommon_names.py\"\n",
    "\n",
    "\n",
    "####################################### Files to import or to modify #########################################################\n",
    "\n",
    "##main path of the data\n",
    "main_path  = '/home/fs01/spec1142/Emma/GateKeepers/'\n",
    "\n",
    "\n",
    "##name cleaning - elements to remove or merge from the names \n",
    "name_del = [\"2nd\", \"3rd\", \"jr\", \"jr.\", \"junior\", \"sr\", \"sr.\", \"senior\", \"i\", 'ii' , 'iii']\n",
    "\n",
    "ln_suff= [\"oster\", \"nordre\", \"vaster\", \"aust\", \"vesle\", \"da\", \"van t\", \"af\", \"al\", \"setya\", \"zu\", \"la\", \"na\", \"mic\", \"ofver\", \"el\", \"vetle\", \"van het\", \"dos\", \"ui\", \"vest\", \"ab\", \"vste\", \"nord\", \"van der\", \"bin\", \"ibn\", \"war\", \"fitz\", \"alam\", \"di\", \"erch\", \"fetch\", \"nga\", \"ka\", \"soder\", \"lille\", \"upp\", \"ua\", \"te\", \"ni\", \"bint\", \"von und zu\", \"vast\", \"vestre\", \"over\", \"syd\", \"mac\", \"nin\", \"nic\", \"putri\", \"bet\", \"verch\", \"norr\", \"bath\", \"della\", \"van\", \"ben\", \"du\", \"stor\", \"das\", \"neder\", \"abu\", \"degli\", \"vre\", \"ait\", \"ny\", \"opp\", \"pour\", \"kil\", \"der\", \"oz\",  \"von\", \"at\", \"nedre\", \"van den\", \"setia\", \"ap\", \"gil\", \"myljom\", \"van de\", \"stre\", \"dele\", \"mck\", \"de\", \"mellom\", \"mhic\", \"binti\", \"ath\", \"binte\", \"snder\", \"sre\", \"ned\", \"ter\", \"bar\", \"le\", \"mala\", \"ost\", \"syndre\", \"sr\", \"bat\", \"sndre\", \"austre\", \"putra\", \"putera\", \"av\", \"lu\", \"vetch\", \"ver\", \"puteri\", \"mc\", \"tre\", \"st\"]\n",
    "\n",
    "\n",
    "## load dictionary with the distribution of the last names: \n",
    "f = open(main_path + \"frequency_last_names.json\",\"r\")\n",
    "import json\n",
    "dic_last_names = json.load(f)\n",
    "\n",
    "## load dictionary with the distribution of the first names: \n",
    "f = open(main_path + \"frequency_first_names.json\",\"r\")\n",
    "import json\n",
    "dic_first_names = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "## load dictionary with the missing institutions (key: display name, values: city, country, longitude, latitude) \n",
    "dic_missing_ids = {}\n",
    "\n",
    "\n",
    "## load dictionary with the missing institutions (key: display name, values: city, country, longitude, latitude) \n",
    "institutions = pd.read_csv(main_path + \"figures/data/institutions_up_to_20230817.tsv\" , delimiter = \"\\t\", index_col = 0 )\n",
    "dic_institutions = institutions.to_dict(\"index\")\n",
    "\n",
    "# extract all the cities from the OpenAlex institutions, as well as their locations (longitude latitude) \n",
    "\n",
    "#flatten the institution dictionary\n",
    "for institution in list(dic_institutions.keys()):\n",
    "    dic_institutions[institution][\"region\"] = dic_institutions[institution][\"region\"]\n",
    "    dic_institutions[institution][\"city\"] = dic_institutions[institution][\"city\"]\n",
    "    dic_institutions[institution][\"longitude\"] = dic_institutions[institution][\"longitude\"]\n",
    "    dic_institutions[institution][\"latitude\"] = dic_institutions[institution][\"latitude\"]\n",
    "    dic_institutions[institution][\"country\"] = dic_institutions[institution][\"country\"]\n",
    "    \n",
    "\n",
    "#store the longitude / latitude cooresponding to the cities\n",
    "dic_cities = {}\n",
    "\n",
    "for institution in list(dic_institutions.keys()):\n",
    "    if dic_institutions[institution][\"city\"] not in dic_cities and dic_institutions[institution][\"latitude\"] != None:\n",
    "        dic_cities[dic_institutions[institution][\"city\"]] = []\n",
    "    if dic_institutions[institution][\"city\"] in dic_cities and [ dic_institutions[institution][\"latitude\"] , dic_institutions[institution][\"longitude\"] ] not in dic_cities[dic_institutions[institution][\"city\"]]:\n",
    "        dic_cities[dic_institutions[institution][\"city\"]].append([ dic_institutions[institution][\"latitude\"] , dic_institutions[institution][\"longitude\"] ])\n",
    "    \n",
    "        \n",
    "\n",
    "#get the list of the cities and of the countries from OpenAlex institution table\n",
    "table = pd.DataFrame(dic_institutions).T    \n",
    "\n",
    "list_cities = set(table[\"city\"].tolist())\n",
    "list_countries = set(table[\"country\"].tolist())\n",
    "list_country_codes = set(table[\"country_code\"].tolist())\n",
    "\n",
    "\n",
    "ps = PorterStemmer()  \n",
    "\n",
    "stop_words = set(stopwords.words('english')) | set(stopwords.words('german'))  | set(stopwords.words('spanish')) | set(stopwords.words('french')) \n",
    "\n",
    "set_institutions_words = {\"univers\" , \"colleg\" , \"hospit\" , \"institut\" , \"research\", \"medicin\" , \"medic\" , \"center\" , \"state\" , \"scienc\" , \"servic\" , \"health\", \"foundat\" , \"corpor\" , \"school\", \"depart\"}\n",
    "\n",
    "words = stop_words | set_institutions_words\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "########################################### Fonctions to load  ###############################################################\n",
    "\n",
    "\n",
    "#merge the particles/suffixes/prefixes with the last name \n",
    "#ln_suff file can be modified if more or less suffixes want to be merged \n",
    "def ln_suff_merge(string):\n",
    "    for suff in ln_suff:\n",
    "        if f\"{' ' + suff + ' '}\" in string or string.startswith(f\"{suff + ' '}\"):\n",
    "            string =  string.replace(f\"{suff + ' '}\", suff.replace(\" \",\"\"))\n",
    "    return string\n",
    "\n",
    "\n",
    "#suppress all the unwanted suffixes from a string\n",
    "#name_del file can be modified if more or less suffixes want to be suppressed \n",
    "def name_delete(string):\n",
    "    for suff in name_del:\n",
    "        if f\"{' ' + suff + ' '}\" in string or string.endswith(f\"{' ' + suff}\"):\n",
    "            string =  string.replace(f\"{suff}\",\"\")\n",
    "    return string\n",
    "\n",
    "\n",
    "#normalize a string dat that represents often a name. \n",
    "def normalize(data):\n",
    "    normal = unicodedata.normalize('NFKD', data).encode('ASCII', 'ignore')\n",
    "    val = normal.decode(\"utf-8\")\n",
    "    # delete unwanted elmt\n",
    "    val = name_delete(val)\n",
    "    # lower full name in upper\n",
    "    val = re.sub(r\"[A-Z]{3,}\", lambda x: x.group().lower(), val)\n",
    "    # add space in front of upper case\n",
    "    if \"Mac\" not in val and \"Mc\" not in val:\n",
    "        val = re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", val)\n",
    "    # Lower case\n",
    "    val = val.lower()\n",
    "    # remove special characters\n",
    "    val = re.sub('[^A-Za-z0-9 -]+', ' ', val)\n",
    "    # remove multiple spaces\n",
    "    val = re.sub(' +', ' ', val)\n",
    "    # remove trailing spaces\n",
    "    val = val.strip()\n",
    "    # suffix merge\n",
    "    val = ln_suff_merge(val)\n",
    "\n",
    "    return val\n",
    "\n",
    "\n",
    "#normalize a string dat that represents an institution. \n",
    "def normalize_inst(data):\n",
    "\n",
    "    # Lower case\n",
    "    data = data.lower()\n",
    "    # remove special characters\n",
    "    data = re.sub('[^A-Za-z0-9 ]+', ' ', data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "#return a ratio of similarity of letters between two strings (to handle in the first names errors)\n",
    "def match_ratio(string1,string2):\n",
    "    return fuzz.ratio(string1, string2)\n",
    "\n",
    "\n",
    "#return 4 if string1 and string2 are the same\n",
    "#return 3 if string1 and string2 sound the same\n",
    "#otherwise, return less\n",
    "def metaphone(string1,string2):\n",
    "    if string1==string2:\n",
    "        return 4\n",
    "    tuple1 = doublemetaphone(string1)\n",
    "    tuple2 = doublemetaphone(string2)\n",
    "    if tuple1[0] == tuple2[0]:\n",
    "        return 3\n",
    "    elif tuple1[0] == tuple2[1] or tuple1[1] == tuple2[0]:\n",
    "        return 2\n",
    "    elif tuple1[1] == tuple2[1]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "#compare name1 and name2. Return 1 if name1 and name2 might represent the same individual, otherwise 0.\n",
    "def comparison(name1 , name2):\n",
    "    \n",
    "    if name1 == name2:\n",
    "        return 1\n",
    "    \n",
    "    #if there is no first name, retrun it's a match\n",
    "    if name1 == \"\" or name2 == \"\":\n",
    "        return 1\n",
    "    \n",
    "    #if some first names exist:\n",
    "    list_name1 = name1.split()\n",
    "    list_name2 = name2.split()\n",
    "    \n",
    "    #minimum number of first names to match\n",
    "    number_match = min( len(list_name1) , len(list_name2))\n",
    "    \n",
    "    #for each name, check if there is a match\n",
    "    count_match = 0\n",
    "    for elem1 in list_name1:\n",
    "        match = 0\n",
    "        \n",
    "        #if we just have the initial:\n",
    "        if len(elem1) == 1:\n",
    "            for elem2 in list_name2:\n",
    "                if elem1[0] == elem2[0]:\n",
    "                    match = 1\n",
    "        else:\n",
    "            for elem2 in list_name2:\n",
    "                #if we just have the initial:\n",
    "                if len(elem2) == 1 and elem1[0] == elem2[0]:\n",
    "                    match = 1\n",
    "                    \n",
    "                #if elem1 and elem2 are entire first names that sound the same and have a ratio of common letters higher thsan 85%, it's a match\n",
    "                elif len(elem2) > 1 and (metaphone(elem1,elem2) > 2 or match_ratio(elem1 , elem2) > 85 ) :\n",
    "                    match = 1\n",
    "                    \n",
    "        #count the number of first names that match    \n",
    "        count_match += match\n",
    "        \n",
    "    #check if we have enough first names that match \n",
    "    if count_match < number_match:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "    \n",
    "## count the number of in common authors\n",
    "def number_of_in_common_authors(paper , patent ):\n",
    "    \n",
    "    authors = paper[\"co_authors\"]\n",
    "\n",
    "    inventors = patent[\"co_inventors\"]\n",
    "    \n",
    "    #count the number of names in common, and store the names in common\n",
    "    count = 0\n",
    "    list_in_common_authors = []\n",
    "    \n",
    "    for name_inventor in inventors:\n",
    "        \n",
    "        for name_author in authors:\n",
    "            \n",
    "            if name_inventor == name_author:\n",
    "                count += 1 \n",
    "                list_in_common_authors.append(name_author + \"-\" + name_inventor)\n",
    "                \n",
    "            \n",
    "            elif len(set(name_inventor.split()) & set( name_author.split())) > 0:\n",
    "                 \n",
    "                match = comparison(name_author , name_inventor)\n",
    "\n",
    "                #if the first names match, we store the first names that are matching and their index \n",
    "                if match == 1:\n",
    "                    count += 1 \n",
    "                    list_in_common_authors.append(name_author + \"-\" + name_inventor)\n",
    "                    \n",
    "\n",
    "\n",
    "    #return 1) the number of names in common, 2) the list of names in common, 3) their index \n",
    "    return  count ,  list_in_common_authors \n",
    "\n",
    "\n",
    "#quantify the similarity between two names \n",
    "def score(author , inventor , name):\n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "    #get the distribution of the last names (from the dictionary dic_last_names)\n",
    "    #it's possible to change the distribution of the last names by changing the dictionary\n",
    "    name = normalize(name)\n",
    "    if name in dic_last_names:\n",
    "        dist = dic_last_names[name]\n",
    "    else:\n",
    "        dist = 0        \n",
    "   \n",
    "    #remove the last name from the author name\n",
    "\n",
    "    author_names = \" \".join([ elem[0] + \" \" + elem[1] if len(elem)==2 else elem for elem in author.split() ]).split()\n",
    "\n",
    "    if name in author_names:\n",
    "        author_names.remove(name)\n",
    "    \n",
    "    inventor_names = inventor.split()\n",
    "\n",
    "    if name in inventor_names:\n",
    "        inventor_names.remove(name)\n",
    "\n",
    "    \n",
    "    #sorte the cleaned author name by the lenght of the first names if there is the initial of the middle name\n",
    "    if len(inventor_names) > 1 and len(inventor_names[1]) > 1 and len(inventor_names[0]) < 3:\n",
    "        inventor_names = sorted(inventor_names, key=len, reverse=True)\n",
    "    if len(author_names) > 1 and len(author_names[1]) > 1 and len(author_names[0]) <3 :\n",
    "        author_names = sorted(author_names , key=len , reverse=True)\n",
    "    \n",
    "  \n",
    "    #if there is not first name, the socre is 0.4\n",
    "    if author_names == [] or inventor_names == []:\n",
    "        score = 0.4\n",
    "\n",
    "\n",
    "    #if both author and inventor have an entire first name (not just initial)\n",
    "    elif len(author_names[0]) > 2 and len(inventor_names[0]) > 2:\n",
    "        \n",
    "        #if the first names match:\n",
    "        if author_names[0] == inventor_names[0]:\n",
    "            \n",
    "            #we add the first name distribution to the distribution of the full name\n",
    "            if author_names[0] in dic_first_names:\n",
    "                dist_first_name = dic_first_names[author_names[0]]\n",
    "            else:\n",
    "                dist_first_name = 0\n",
    "                \n",
    "            #if a middle name match, the score is 1     \n",
    "            if len(author_names) > 1 and len(inventor_names) > 1 and author_names[1][0] == inventor_names[1][0]:\n",
    "                dist = dist*dist_first_name\n",
    "                score = 1\n",
    "            \n",
    "            #if there is no middle name, the score is 0.8\n",
    "            else:\n",
    "                dist = dist*dist_first_name\n",
    "                score = 0.8\n",
    "                \n",
    "        #if the first names don't match:\n",
    "        else:\n",
    "            \n",
    "            #if the first names sound the same and have more than 85% of letters in common, the score is 0.7\n",
    "            if (metaphone(author_names[0],inventor_names[0])) > 2 or match_ratio(author_names[0],inventor_names[0]) > 85 :\n",
    "                \n",
    "                #we add the first name distribution to the distribution of the full name\n",
    "                if inventor_names[0] in dic_first_names:\n",
    "                    dist_first_name = dic_first_names[inventor_names[0]]\n",
    "                else:\n",
    "                    dist_first_name = 0\n",
    "                dist = dist*dist_first_name\n",
    "                score = 0.7\n",
    "                    \n",
    "            #else, the score is 0.1\n",
    "            else:\n",
    "                score = 0.1\n",
    "    \n",
    "    #if the author or the inventor only have an initial:\n",
    "    \n",
    "    elif len(author_names[0]) < 3 or len(inventor_names[0]) < 3:\n",
    "\n",
    "        inventor_names = inventor.split()\n",
    "        #if only the first initial of the author matches with the first initial of the inventor, the score is 0.6\n",
    "\n",
    "        if author_names[0][0] == inventor_names[0][0]:\n",
    "            score = 0.6\n",
    "            \n",
    "            \n",
    "            #if more than one initial are matching, the score is 0.8\n",
    "            if len(author_names) > 1 and len(inventor_names) > 1 and author_names[1][0] == inventor_names[1][0]:\n",
    "                score = 0.8\n",
    "        #if only a middle initial matches with am initial, the score is 0.2\n",
    "        else:\n",
    "            score = 0.2\n",
    "    \n",
    "    #return the similarity between the inventor and author name, the distribution of the matching name, \n",
    "    #the author and inventor names and the similarity between the inventor and author name normalize by the distribution of the name. \n",
    "    return   score, dist,  author  , inventor , score /(1 + dist)\n",
    "    \n",
    "        \n",
    "    \n",
    "## calculate efficiently the dot product between two vectors\n",
    "def norm(vector):\n",
    "    return sqrt(sum(x * x for x in vector))    \n",
    "\n",
    "def cosine_similarity2(vec_a, vec_b):\n",
    "        norm_a = norm(vec_a)\n",
    "        norm_b = norm(vec_b)\n",
    "        dot = sum(a * b for a, b in zip(vec_a, vec_b))\n",
    "        return dot / (norm_a * norm_b)\n",
    "     \n",
    "       \n",
    "## tranform string into vector\n",
    "def clean_encoding(encoded_text):\n",
    "    if encoded_text == None:\n",
    "        return None\n",
    "    else:\n",
    "        if \"\\n\" in encoded_text:\n",
    "            encoded_text = encoded_text.replace(\"\\n\" , \"\")\n",
    "        encoded_text = encoded_text[1:-1]\n",
    "        encoded_text = list(map(float , encoded_text.split()))\n",
    "        return encoded_text\n",
    "    \n",
    "    \n",
    "## quantify similarity between the titles (with BERT) \n",
    "def similarity_title(paper , patent):\n",
    "    \n",
    "    #return the similarity between the titles if exist, else return none\n",
    "    if patent[\"encoded_title\"] == None or paper[\"encoded_title\"] == None or patent[\"encoded_title\"] == [] or paper[\"encoded_title\"] == []:\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        return cosine_similarity2(patent[\"encoded_title\"], paper[\"encoded_title\"])\n",
    "\n",
    "    \n",
    "## quantify similarity between the abstract (with BERT) \n",
    "def similarity_abstract(paper , patent):\n",
    "    \n",
    "    #return the similarity between the abstracts if exist, else return none\n",
    "    if patent[\"encoded_abstract\"] == None or paper[\"encoded_abstract\"] == None or patent[\"encoded_abstract\"] == [] or paper[\"encoded_abstract\"] == []:\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        return cosine_similarity2(patent[\"encoded_abstract\"], paper[\"encoded_abstract\"])\n",
    "\n",
    "    \n",
    "\n",
    "#get the location from an institution mame\n",
    "def get_location_display_name(display_name):\n",
    "    #clean the institution name\n",
    "    display_name = ''.join([i for i in display_name if not i.isdigit()])\n",
    "    display_name = display_name.replace(\"*\" , \"\")\n",
    "    display_name = display_name.replace(\"#TAB#\" , \"\")\n",
    "    display_name = display_name.replace(\"  \" , \" \")\n",
    "    \n",
    "    #extract the location and organisation from the institution name\n",
    "    doc = spacy_nlp(display_name.strip())\n",
    "    location_entities= set()\n",
    "    organization_entities = set()\n",
    "    for i in doc.ents:\n",
    "        entry = str(i.lemma_).lower()\n",
    "        display_name = display_name.replace(str(i).lower(), \"\")\n",
    "        if i.label_ in [\"GPE\", \"GEO\",\"LOC\"]:\n",
    "            location_entities.add(i)\n",
    "        if i.label_ in [\"ORG\"]:\n",
    "            organization_entities.add(i)\n",
    "            \n",
    "    #return         \n",
    "    return location_entities , organization_entities\n",
    "\n",
    "\n",
    "#get the coordinates from an institution name\n",
    "def get_location_missing( display_name):\n",
    "    \n",
    "    # extract all the cities from the OpenAlex institutions, as well as their locations (longitude latitude) \n",
    "\n",
    "    #get the location from an institution name\n",
    "    dic = {}\n",
    "    location = get_location_display_name(display_name)[0]\n",
    "    cities = set()\n",
    "    countries = set()\n",
    "                    \n",
    "                    \n",
    "    #search a cooresponding city in OpenAlex institution's cities                \n",
    "    for elem in location:\n",
    "        elem = str(elem).title()\n",
    "                    \n",
    "        if elem in list_cities and elem in dic_cities:\n",
    "            cities.add(elem)\n",
    "\n",
    "        if elem in list_countries or elem in list_country_codes:\n",
    "            countries.add(elem)\n",
    "                \n",
    "    dic[\"city\"] = list(cities - countries)\n",
    "    dic[\"country\"] = list(countries)\n",
    "    \n",
    "    #get the coordinates associated with a city\n",
    "    if cities != set():\n",
    "        dic[\"longitude\"] = float(dic_cities[list(cities)[0]][0][1])\n",
    "        dic[\"latitude\"] = float(dic_cities[list(cities)[0]][0][0])\n",
    "        \n",
    "    #return a dictionary where the key is the institution names and the values are the city, country, longitude, latitude\n",
    "    return dic\n",
    "\n",
    "\n",
    "\n",
    "## calculate efficiently the geographic distance between two points on the earth\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    # Radius of earth in kilometers is 6371\n",
    "    km = 6371* c\n",
    "    return km\n",
    "\n",
    "\n",
    "\n",
    "##calculate the minimal distance between the paper and the patent assignee\n",
    "def distance_assignees(paper , patent , data_author):\n",
    "     \n",
    "    #create the list of coordinates corresponding to the paper and the patent assignee\n",
    "    coords_patents = list(set([ (long, lat) for long,lat in zip(patent[\"assignee_longitude\"], patent[\"assignee_latitude\"]) ]))\n",
    "    coords_papers = list(set([ (long, lat) for long,lat in zip(paper[\"longitude\"], paper[\"latitude\"]) ]))\n",
    "\n",
    "    #if there is no coordinates corresponding to the paper, take the last institution of the author\n",
    "    if coords_papers == []:\n",
    "        coords_papers = list(set([ (long, lat) for long,lat in zip(data_author[\"longitude\"], data_author[\"latitude\"]) ]))\n",
    "\n",
    "    if coords_patents == [] or coords_papers ==[]:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    #calculate the minimal distance\n",
    "    else: \n",
    "        distance = np.inf\n",
    "        for coord_patent in coords_patents:\n",
    "            \n",
    "            if coord_patent in coords_papers:\n",
    "                return 0\n",
    "            \n",
    "            else:\n",
    "                for coord_paper in coords_papers:\n",
    "                    if coord_patent[0] != None and coord_patent[1] != None and coord_paper[0] != None and coord_paper[1] != None:\n",
    "                        dist = haversine(coord_patent[0], coord_patent[1], coord_paper[0], coord_paper[1])\n",
    "                        if dist < distance:\n",
    "                            distance = dist\n",
    "\n",
    "\n",
    "    if distance == np.inf:\n",
    "        return None\n",
    "    \n",
    "    #return the minimal distance\n",
    "    return distance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##calculate the minimal distance between the paper and the patent inventors\n",
    "def distance_inventors(paper , patent , data_author):\n",
    "    \n",
    "    #create the list of coordinates corresponding to the paper and the patent inventors\n",
    "    coords_patents = [(patent[\"inventor_longitude\"], patent[\"inventor_latitude\"])]\n",
    "    coords_papers = list(set([ (long, lat) for long,lat in zip(paper[\"longitude\"], paper[\"latitude\"]) ]))\n",
    "\n",
    "    #if there is no coordinates corresponding to the paper, take the last institution of the author\n",
    "    if  coords_papers == []:\n",
    "        coords_papers = list(set([ (long, lat) for long,lat in zip(data_author[\"longitude\"], data_author[\"latitude\"]) ]))\n",
    "\n",
    "    if coords_patents == [] or coords_papers ==[]:\n",
    "        return None\n",
    "    \n",
    "    #calculate the minimal distance\n",
    "    else: \n",
    "        distance = np.inf\n",
    "        for coord_patent in coords_patents:\n",
    "            \n",
    "            if coord_patent in coords_papers:\n",
    "                return None\n",
    "            \n",
    "            else:\n",
    "                for coord_paper in coords_papers:\n",
    "                    if coord_patent[0] != None and coord_patent[1] != None and coord_paper[0] != None and coord_paper[1] != None:\n",
    "                        dist = haversine(coord_patent[0], coord_patent[1], coord_paper[0], coord_paper[1])\n",
    "                        if dist < distance:\n",
    "                            distance = dist\n",
    "\n",
    "    if distance == np.inf:\n",
    "        return None\n",
    "        \n",
    "    #return the minimal distance\n",
    "    return distance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#clean institution string\n",
    "def clean_institution_name(institution):\n",
    "    \n",
    "    institution = set(normalize_inst(institution).split())\n",
    "    set_institution = set([ps.stem(word) for word in institution if word not in words ])\n",
    "\n",
    "    return set_institution\n",
    "\n",
    "\n",
    "#count the number of words in common between the cleaned patent and the paper's institutions\n",
    "def similarity_institution_name(paper, patent , data_author):\n",
    "    \n",
    "    assignee = patent[\"assignee_organization\"]\n",
    "    if assignee == None:\n",
    "        return 0\n",
    "    \n",
    "    else:\n",
    "        assignee = clean_institution_name(assignee)\n",
    "        \n",
    "    institution = paper[\"institution_name\"]\n",
    "    if institution == None:\n",
    "        institution = data_author[\"last_known_institution_display_name\"]\n",
    "        \n",
    "    if institution == None:\n",
    "        return 0\n",
    "    \n",
    "    else:\n",
    "        institution = clean_institution_name(institution)\n",
    "        \n",
    "    return len(institution & assignee)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "#get number of in common citing or cited patents \n",
    "def in_common_citing_papers(paper , patent ):\n",
    "    #return the number of papers that are cited by the selected patent and by selected paper \n",
    "    \n",
    "    referenced_paper = paper[\"referenced_works\"]\n",
    "    cited_papers = patent[\"cited_papers\"]\n",
    "    \n",
    "    if referenced_paper == None or cited_papers == None:\n",
    "        return 0\n",
    " \n",
    "    else:\n",
    "        return len( cited_papers & referenced_paper ) \n",
    "    \n",
    "\n",
    "\n",
    "from scipy.stats import chi2\n",
    "#calculate the likelihood of the patent in the papers dates distributions\n",
    "def calculate_likelihood_chi2(dic_dates, patent):\n",
    "    patent_date = patent[\"patent_date\"].year\n",
    "    df, loc, scale = dic_dates[\"chi_dist\"] \n",
    "    res = chi2.logpdf(patent_date, df, loc, scale)\n",
    "    if res < -5000:\n",
    "        res = -5000\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def comparison_pairs2(dic_comparison , paper , patent , data_author , dic_dates):\n",
    "    \n",
    "    #dictionary that regroup all the comparison feathures between a paper and a patent\n",
    "    #key: paper number + space + patent number\n",
    "\n",
    "    dic_comparison[\"number_in_common_authors\"] , dic_comparison[\"list_in_common_authors\"]   = number_of_in_common_authors(paper , patent)\n",
    "\n",
    "    dic_comparison[\"distance_assignees\"] = distance_assignees(paper , patent , data_author)\n",
    "    dic_comparison[\"distance_inventors\"] = distance_inventors(paper , patent , data_author)\n",
    "\n",
    "    dic_comparison[\"in_common_citing_papers\"] = in_common_citing_papers(paper , patent )\n",
    "\n",
    "    #dic_comparison[\"in_common_citing_or_cited_patents\"] = in_common_citing_or_cited_patents(paper , patent )\n",
    "    #dic_comparison[\"similarity_cpcs_wipos_concepts\"] = similarity_cpcs_wipos_concepts(paper , patent)\n",
    "\n",
    "    dic_comparison[\"similarity_institution\"] = similarity_institution_name(paper , patent , data_author)\n",
    "\n",
    "    if dic_dates != None: \n",
    "        dic_comparison[\"date_likelihood\"] = calculate_likelihood_chi2(dic_dates, patent)\n",
    "   \n",
    "        dic_comparison[\"date_difference\"] = abs(patent[\"patent_date\"].year - dic_dates[\"mean_dates\"])\n",
    "        dic_comparison[\"publications_range\"] = dic_dates[\"max_dates\"] - dic_dates[\"min_dates\"]\n",
    "        dic_comparison[\"number_publications\"] = len(dic_dates[\"list_dates\"])\n",
    "    else:\n",
    "        dic_comparison[\"date_likelihood\"] = None\n",
    "        dic_comparison[\"date_difference\"] = None\n",
    "        dic_comparison[\"publications_range\"] = None\n",
    "        dic_comparison[\"number_publications\"] = None\n",
    "\n",
    "\n",
    "    return dic_comparison\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df28578e-67c6-4c96-bf90-b2bfa45c8fff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Get inventors and authors ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf5964b-5258-4f2e-b8f5-4a09889afe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from a last name, gives the PatentsView ids, patent numbers and first names that correspond to the given last name.\n",
    "\n",
    "def get_PatentsView_inventors_ids(last_name):\n",
    "    \n",
    "    #query the ids, patent numbers, first names that correspond to the last name.\n",
    "    \n",
    "\n",
    "    #establishing the connection\n",
    "    conn = psycopg2.connect(\"user=\" + user + \" password=\" + password)\n",
    "\n",
    "    #Creating a cursor object using the cursor() method\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    last_name = last_name.lower()\n",
    "    \n",
    "    text = \"\"\"SELECT   i.inventor_id , \n",
    "                       i.disambig_inventor_name_first , \n",
    "                       i.disambig_inventor_name_last \n",
    "\n",
    "              FROM inventors_PatentsView as i\n",
    "\n",
    "              WHERE f_unaccent(i.disambig_inventor_name_last) ILIKE '% \"\"\" + last_name + \"\"\" %'\n",
    "              \n",
    "              OR f_unaccent(i.disambig_inventor_name_last) ILIKE '\"\"\" + last_name + \"\"\" %'\n",
    "\n",
    "              OR f_unaccent(i.disambig_inventor_name_last) ILIKE '\"\"\" + last_name + \"\"\"'\n",
    "              \n",
    "              OR f_unaccent(i.disambig_inventor_name_last) ILIKE '% \"\"\" + last_name + \"\"\"'\n",
    "              \n",
    "              ;\"\"\"\n",
    "\n",
    "    cursor.execute(text)\n",
    "\n",
    "    res = cursor.fetchall()\n",
    "\n",
    "\n",
    "    #Closing the connection\n",
    "    conn.close()\n",
    "\n",
    "    \n",
    "    dic_inventors = {}\n",
    "    for line in res:\n",
    "\n",
    "        inventor_id = line[0]\n",
    "        \n",
    "        if \"'\" in inventor_id:\n",
    "            inventor_id = inventor_id.replace( \"'\" , \"''\")\n",
    "            \n",
    "        dic_inventors[inventor_id] = {}\n",
    "        inventor_first_name = line[1]\n",
    "        if inventor_first_name == None:\n",
    "            inventor_first_name = ''\n",
    "\n",
    "        dic_inventors[inventor_id][\"inventor_first_name\"] = normalize(inventor_first_name)\n",
    "        dic_inventors[inventor_id][\"inventor_last_name\"] = normalize(line[2])\n",
    "\n",
    "\n",
    "    dic_inventor_first_names = {}\n",
    "    for inventor_id in list(dic_inventors.keys()):\n",
    "        first_names = dic_inventors[inventor_id][\"inventor_first_name\"].split()\n",
    "\n",
    "        for first_name in first_names:\n",
    "\n",
    "            if first_name != '' and first_name not in dic_inventor_first_names:\n",
    "                dic_inventor_first_names[first_name] = []\n",
    "            dic_inventor_first_names[first_name].append(inventor_id)\n",
    "                             \n",
    "\n",
    "    #return a list of ids, first names that correspond to the given last name\n",
    "    return dic_inventors , dic_inventor_first_names\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from a last name, gives the PatentsView ids, patent numbers and first names that correspond to the given last name.\n",
    "\n",
    "def get_OpenAlex_author_ids(last_name , first_name):\n",
    "    \n",
    "    #query the ids, patent numbers, first names that correspond to the last name.\n",
    "    \n",
    "    #establishing the connection\n",
    "    conn = psycopg2.connect(\"user=\" + user + \" password=\" + password)\n",
    "\n",
    "    #Creating a cursor object using the cursor() method\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    if first_name == '':\n",
    "        \n",
    "        \n",
    "        text = \"\"\"SELECT   a.author_id ,\n",
    "                       a.display_name , \n",
    "                       a.orcid , \n",
    "                       a.last_known_institution_id , \n",
    "                       a.last_known_institution_display_name\n",
    "                       \n",
    "          \n",
    "          FROM authors_OpenAlex AS a\n",
    "                    \n",
    "          WHERE f_unaccent(a.display_name) ILIKE '\"\"\" + last_name + \"\"\" %'\n",
    "          \n",
    "          OR f_unaccent(a.display_name) ILIKE ' \"\"\" + last_name + \"\"\"%'\n",
    "\n",
    "          OR f_unaccent(a.display_name) ILIKE '% \"\"\" + last_name + \"\"\" %';\"\"\"\n",
    "        \n",
    "    \n",
    "    else:\n",
    "\n",
    "\n",
    "        first_name = first_name.lower()\n",
    "        last_name = last_name.lower()\n",
    "    \n",
    "        text = \"\"\"SELECT   a.author_id ,\n",
    "                           a.display_name , \n",
    "                           a.orcid , \n",
    "                           a.last_known_institution_id , \n",
    "                           a.last_known_institution_display_name\n",
    "                           \n",
    "\n",
    "              FROM authors_OpenAlex AS a\n",
    "\n",
    "              WHERE ( f_unaccent(a.display_name) ILIKE '\"\"\" + last_name + \"\"\" %'\n",
    "          \n",
    "              OR f_unaccent(a.display_name) ILIKE '% \"\"\" + last_name + \"\"\"'\n",
    "    \n",
    "              OR f_unaccent(a.display_name) ILIKE '% \"\"\" + last_name + \"\"\" %') \n",
    "\n",
    "              AND ( f_unaccent(a.display_name) ILIKE '\"\"\" + first_name + \"\"\" %'\n",
    "\n",
    "                    OR f_unaccent(a.display_name) ILIKE '% \"\"\" + first_name + \"\"\"'\n",
    "\n",
    "                    OR f_unaccent(a.display_name) ILIKE '% \"\"\" + first_name + \"\"\" %'\n",
    "\n",
    "                    OR f_unaccent(a.display_name) ILIKE '\"\"\" + first_name[0] + \"\"\" %'\n",
    "\n",
    "                    OR f_unaccent(a.display_name) ILIKE '\"\"\" + first_name[0] + \"\"\".%'\n",
    "                    \n",
    "                    );\"\"\"\n",
    "\n",
    "        \n",
    "    cursor.execute(text)\n",
    "    res = cursor.fetchall()\n",
    "\n",
    "\n",
    "    #Closing the connection\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "    dic_author = {}\n",
    "    for line in res:\n",
    "\n",
    "        author_id = line[0]\n",
    "        dic_author[author_id] = {}\n",
    "        dic_author[author_id][\"author_name\"] = normalize(line[1])\n",
    "        dic_author[author_id][\"orcid\"] = line[2]\n",
    "        dic_author[author_id][\"last_known_institution_id\"] = line[3]\n",
    "        dic_author[author_id][\"last_known_institution_display_name\"] = line[4]\n",
    "        \n",
    "        if line[3] in dic_institutions:\n",
    "            data = dic_institutions[line[3]]\n",
    "            dic_author[author_id][\"longitude\"] = [data[\"longitude\"]]\n",
    "            dic_author[author_id][\"latitude\"] = [data[\"latitude\"]]\n",
    "        else:\n",
    "            dic_author[author_id][\"longitude\"] = []\n",
    "            dic_author[author_id][\"latitude\"] = []\n",
    "            \n",
    "\n",
    "\n",
    "    #return a list of ids, first names that correspond to the given last name\n",
    "    return dic_author\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Display a matrix with the inventor names and author names. \n",
    "#For the author and inventor names that might represent the same person, the score is greater than 0\n",
    "\n",
    "def get_pd_table_comparison(last_name):\n",
    "    \n",
    "    ## dic_comparison just to get the DataFrame\n",
    "    dic_comparison = {}\n",
    "    \n",
    "    ##store the ids of the match\n",
    "    dic_ids = {}\n",
    "    \n",
    "    ## query the authors and the inventors corresponding to the last name\n",
    "    \n",
    "    dic_inventors, dic_inventor_first_names = get_PatentsView_inventors_ids(last_name)\n",
    "    \n",
    "        \n",
    "    dic_authors_full  = {}\n",
    "\n",
    "    \n",
    "    for first_name in list(dic_inventor_first_names.keys()):\n",
    "    \n",
    "        dic_authors = get_OpenAlex_author_ids(last_name , first_name)        \n",
    "        \n",
    "        dic_authors_full = { **dic_authors_full , **dic_authors }\n",
    "    \n",
    "        ## normalize last name\n",
    "        last_name_norm = normalize(last_name)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        ##for each author id, \n",
    "        \n",
    "        #start = time.time()\n",
    "        \n",
    "        for author_id in dic_authors:\n",
    "            \n",
    "\n",
    "            ## extract the first names of the author\n",
    "            name_norm = dic_authors[author_id][\"author_name\"]\n",
    "            first_names = normalize(name_norm.replace(last_name_norm , \"\"))\n",
    "\n",
    "            ## separate the first name initials\n",
    "            if name_norm.split()[0] == last_name_norm and len(first_names) < 4:\n",
    "                first_names = normalize(first_names.replace(\"\" , \" \"))\n",
    "\n",
    "            ## get the first names\n",
    "            dic_authors[author_id][\"author_first_names\"] = first_names\n",
    "\n",
    "            ## get the author name\n",
    "            name = dic_authors[author_id][\"author_first_names\"]  + \" \" + last_name_norm\n",
    "            \n",
    "            if name not in dic_comparison:\n",
    "                dic_comparison[name] = {}\n",
    "\n",
    "            first_names_OA = dic_authors[author_id][\"author_first_names\"]\n",
    "            \n",
    "\n",
    "            ## compare the author name with the inventor's name\n",
    "            for inventor_id in dic_inventor_first_names[first_name]:\n",
    "\n",
    "                first_names_PV = dic_inventors[inventor_id][\"inventor_first_name\"]\n",
    "                last_names_PV = dic_inventors[inventor_id][\"inventor_last_name\"]\n",
    "\n",
    "\n",
    "                #compare the OA first names and the PV first names. \n",
    "                # add the similarity score into dic_comparison: the first key is the author name, the second key is the inventor name. \n",
    "                \n",
    "                if comparison(first_names_OA , first_names_PV) == 1:\n",
    "                    \n",
    "                    res =  score(name, first_names_PV + \" \" + last_name_norm , last_name_norm)\n",
    "                    \n",
    "\n",
    "                    inventor_author_pair = name + \"%\" + first_names_PV + \" \" + last_names_PV\n",
    "                    \n",
    "                    ## store the normalize simiarity score\n",
    "                    dic_comparison[name][first_names_PV + \" \" + last_names_PV] = res[-1]\n",
    "                        \n",
    "                    \n",
    "                    if res[-1]**2 + 1 > 1.25:\n",
    "                        \n",
    "                                                \n",
    "                        if inventor_author_pair not in dic_ids:\n",
    "\n",
    "                            dic_ids[inventor_author_pair] = {}\n",
    "                            dic_ids[inventor_author_pair][\"OpenAlex_id\"] = set()\n",
    "                            dic_ids[inventor_author_pair][\"PatentsView_id\"] = set()\n",
    "                            dic_ids[inventor_author_pair][\"Raw_score\"] = res[0]\n",
    "                            dic_ids[inventor_author_pair][\"Norm_score\"] = res[-1]\n",
    "\n",
    "                        if author_id not in dic_ids[inventor_author_pair][\"OpenAlex_id\"]:\n",
    "                            dic_ids[inventor_author_pair][\"OpenAlex_id\"].add(author_id)\n",
    "\n",
    "                        if inventor_id not in dic_ids[inventor_author_pair][\"PatentsView_id\"]:\n",
    "                            dic_ids[inventor_author_pair][\"PatentsView_id\"].add(inventor_id)\n",
    "\n",
    "                    else:\n",
    "                        dic_comparison[name][first_names_PV + \" \" + last_names_PV] = 0\n",
    "                            \n",
    "\n",
    "                else:\n",
    "                    dic_comparison[name][first_names_PV + \" \" + last_names_PV] = 0\n",
    "                    \n",
    "        #end = time.time()\n",
    "        #print(end - start)\n",
    "\n",
    "\n",
    "\n",
    "    #return 1) table a DataFrame with the inventor and author names and their similarity scores\n",
    "    #return 2) dic_comparison a dictionary with the author name (first key) and inventor name (second key) and their similarity scores (values) \n",
    "    #returm 3) and 4) the list of patents_ids and the list of author ids updated\n",
    "    return dic_comparison , dic_ids , dic_authors_full , dic_inventors\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5099b1a-35ae-4974-ba19-c44d98e32d64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jennifer a doudna</th>\n",
       "      <th>james h doudna cate</th>\n",
       "      <th>david doudna</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>jennifer a doudna</th>\n",
       "      <td>0.787591</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j a doudna</th>\n",
       "      <td>0.579620</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j h doudna</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.579620</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j ch doudna</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>james h cate doudna</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.743397</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h e doudna</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d doudna</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     jennifer a doudna  james h doudna cate  david doudna\n",
       "jennifer a doudna             0.787591                  NaN           NaN\n",
       "j a doudna                    0.579620             0.000000           NaN\n",
       "j h doudna                    0.000000             0.579620           NaN\n",
       "j ch doudna                   0.000000             0.000000           NaN\n",
       "james h cate doudna                NaN             0.743397           NaN\n",
       "h e doudna                         NaN             0.000000           NaN\n",
       "d doudna                           NaN                  NaN           0.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = get_pd_table_comparison(\"doudna\")\n",
    "pd.DataFrame(table[0]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55db1492-e0d6-4e44-9f8f-bb2b1a6a1541",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Get patents and papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2845aa97-c8b5-46e0-b9f0-8efa15e1d73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the patents associated with the patent numbers stored in the dictionary dic_ids\n",
    "\n",
    "def get_patents(dic_ids):\n",
    "    \n",
    "    dic_patents = {}\n",
    "    \n",
    "    #establishing the connection\n",
    "    conn = psycopg2.connect(\"user=\" + user + \" password=\" + password)\n",
    "\n",
    "    #Creating a cursor object using the cursor() method\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    \n",
    "    for names in dic_ids:\n",
    "        inventor_name = names.split(\"%\")[1]\n",
    "        set_inventor_ids = dic_ids[names][\"PatentsView_id\"]\n",
    "        for inventor_id in set_inventor_ids:           \n",
    "                \n",
    "            if inventor_id not in dic_patents:\n",
    "        \n",
    "                dic_patents[inventor_id] = {}\n",
    "                \n",
    "                text = \"\"\"SELECT i.patent_id, \n",
    "                                 i.disambig_inventor_name_first ,\n",
    "                                 i.disambig_inventor_name_last ,\n",
    "                                 li.longitude,\n",
    "                                 li.latitude,\n",
    "                                 p.patent_date,\n",
    "                                 pe.encoded_title , \n",
    "                                 pe.encoded_abstract , \n",
    "                                 string_agg(a.disambig_assignee_organization, '%')  , \n",
    "                                 string_agg( CAST(la.longitude AS VARCHAR), ','),\n",
    "                                 string_agg(CAST(la.latitude AS VARCHAR), ','),\n",
    "                                 string_agg( CONCAT(co.disambig_inventor_name_first , ' ' ,  co.disambig_inventor_name_last) , ','  ) ,\n",
    "                                 string_agg(npc.work_id , ',')\n",
    "                                 \n",
    "           \n",
    "                          FROM inventors_PatentsView AS i\n",
    "                          LEFT JOIN patents_PatentsView AS p ON  i.patent_id  = p.patent_id\n",
    "                          LEFT JOIN locations_PatentsView AS li ON  i.location_id  = li.location_id\n",
    "                          LEFT JOIN assignees_PatentsView AS a ON  i.patent_id = a.patent_id\n",
    "                          LEFT JOIN locations_PatentsView AS la ON a.location_id = la.location_id\n",
    "                          LEFT JOIN inventors_PatentsView AS co ON i.patent_id = co.patent_id\n",
    "                          LEFT JOIN non_patent_citations_matt_marx AS npc ON CONCAT('us-' , i.patent_id) = npc.patent_id\n",
    "                          LEFT JOIN encoded_patents_patentsview AS pe ON  i.patent_id = pe.patent_id\n",
    "                          \n",
    "                          WHERE i.inventor_id = '\"\"\" + inventor_id + \"\"\"'\n",
    "\n",
    "                          GROUP BY i.patent_id,\n",
    "                                 i.disambig_inventor_name_first ,\n",
    "                                 i.disambig_inventor_name_last ,\n",
    "                                 li.longitude,\n",
    "                                 li.latitude,\n",
    "                                 p.patent_date,\n",
    "                                 pe.encoded_title , \n",
    "                                 pe.encoded_abstract\"\"\"\n",
    "\n",
    "                cursor.execute(text)\n",
    "                res = cursor.fetchall()\n",
    "                \n",
    "                for line in res:\n",
    "                    \n",
    "                    \n",
    "                    patent_id = line[0]\n",
    "                    dic_patents[inventor_id][patent_id] = {}\n",
    "                    dic_patents[inventor_id][patent_id][\"inventor_id\"] = inventor_id\n",
    "                    dic_patents[inventor_id][patent_id][\"inventor_first_name\"] = line[1]\n",
    "                    dic_patents[inventor_id][patent_id][\"inventor_last_name\"] = line[2]\n",
    "                    dic_patents[inventor_id][patent_id][\"inventor_longitude\"] = line[3]\n",
    "                    dic_patents[inventor_id][patent_id][\"inventor_latitude\"] = line[4]\n",
    "                    dic_patents[inventor_id][patent_id][\"patent_date\"] = line[5]\n",
    "                    \n",
    "                    try:\n",
    "                        dic_patents[inventor_id][patent_id][\"encoded_title\"] = clean_encoding(line[6])\n",
    "                        dic_patents[inventor_id][patent_id][\"encoded_abstract\"] = clean_encoding(line[7])\n",
    "                    except:\n",
    "                        dic_patents[inventor_id][patent_id][\"encoded_title\"] = None\n",
    "                        dic_patents[inventor_id][patent_id][\"encoded_abstract\"] = None\n",
    "                \n",
    "                        \n",
    "                        \n",
    "                    if line[8] != None:\n",
    "                        dic_patents[inventor_id][patent_id][\"assignee_organization\"] = \"; \".join(list(set(line[8].split(\"%\"))))\n",
    "                    else:\n",
    "                        dic_patents[inventor_id][patent_id][\"assignee_organization\"] = line[8]\n",
    "                    if line[9] != None:\n",
    "                        dic_patents[inventor_id][patent_id][\"assignee_longitude\"] = list(set([ float(elem) for elem in line[9].split(\",\")]))\n",
    "                        dic_patents[inventor_id][patent_id][\"assignee_latitude\"] = list(set([ float(elem) for elem in line[10].split(\",\")]))\n",
    "                    else:\n",
    "                        dic_patents[inventor_id][patent_id][\"assignee_longitude\"] = []\n",
    "                        dic_patents[inventor_id][patent_id][\"assignee_latitude\"] = []\n",
    "                        \n",
    "                    dic_patents[inventor_id][patent_id][\"co_inventors\"] = [ normalize(elem) for elem in set(line[11].split(\",\")) ] \n",
    "                    if line[12] != None:\n",
    "                        dic_patents[inventor_id][patent_id][\"cited_papers\"] = set(line[12].split(\",\"))\n",
    "                    else:\n",
    "                        dic_patents[inventor_id][patent_id][\"cited_papers\"]  = None\n",
    "                        \n",
    "                        \n",
    "                    dic_patents[inventor_id][patent_id][\"inventor_name\"]  = inventor_name\n",
    "                    \n",
    "                    \n",
    "    \n",
    "    #Closing the connection\n",
    "    conn.close()\n",
    "    \n",
    "    return dic_patents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_time(dic_ids ):\n",
    "    \n",
    "    dic_patents = get_patents(dic_ids)\n",
    "    \n",
    "    dic_good_ids = {}\n",
    "    dic_encoding = {}\n",
    "    \n",
    "    dic_comparison = {}\n",
    "    dic_papers = {}\n",
    "    \n",
    "    #establishing the connection\n",
    "    conn = psycopg2.connect(\"user=\" + user + \" password=\" + password)\n",
    "\n",
    "    #Creating a cursor object using the cursor() method\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    for name in dic_ids:\n",
    "     \n",
    "        \n",
    "        OpenAlex_ids = dic_ids[name]['OpenAlex_id']\n",
    "        PatentsView_id = dic_ids[name]['PatentsView_id']\n",
    "                                \n",
    "            \n",
    "        for author_id in OpenAlex_ids:\n",
    "\n",
    "            if author_id not in dic_encoding:\n",
    "                \n",
    "                dic_comparison[author_id] = {}\n",
    "                dic_papers[author_id] = {}\n",
    "                dic_encoding[author_id] = {}\n",
    "                \n",
    "\n",
    "                text = \"\"\" SELECT we.work_id,  we.encoded_title , we.encoded_abstract , string_agg(wa.institution_id , ',') , string_agg(wa.institution_name, ',')\n",
    "                            FROM works_authors_OpenAlex AS wa\n",
    "                            LEFT JOIN encoded_works_openalex AS we ON wa.work_id = we.work_id\n",
    "                            WHERE wa.author_id = '\"\"\"+ str(author_id) +\"\"\"'\n",
    "                            GROUP BY we.work_id,  we.encoded_title , we.encoded_abstract\n",
    "                            ;\"\"\"\n",
    "                \n",
    "                cursor.execute(text)\n",
    "                res = cursor.fetchall()\n",
    "                \n",
    "                for line in res:\n",
    "                    work_id = line[0]\n",
    "                    \n",
    "                    \n",
    "                    if work_id != None and work_id not in dic_encoding[author_id]:\n",
    "                        dic_papers[author_id][work_id] = {}\n",
    "                        \n",
    "                        \n",
    "                        dic_papers[author_id][work_id][\"institution_name\"] = line[4]\n",
    "                        \n",
    "                        dic_papers[author_id][work_id][\"longitude\"] = []\n",
    "                        dic_papers[author_id][work_id][\"latitude\"] = []\n",
    "                        \n",
    "                        if line[3] != None:\n",
    "                            institution_ids = line[3].split(\",\")\n",
    "                            for institution_id in institution_ids:\n",
    "                                if institution_id in dic_institutions:\n",
    "                                    data = dic_institutions[institution_id]\n",
    "                                    dic_papers[author_id][work_id][\"longitude\"] += [data[\"longitude\"]]\n",
    "                                    dic_papers[author_id][work_id][\"latitude\"] += [data[\"latitude\"]]\n",
    "                                    dic_papers[author_id][work_id][\"institution_name\"] = data[\"display_name\"]\n",
    "\n",
    "\n",
    "\n",
    "                        institution_name = dic_papers[author_id][work_id][\"institution_name\"]\n",
    "                        if dic_papers[author_id][work_id][\"longitude\"] == [] and institution_name != None and institution_name != \"\":\n",
    "                            if institution_name in dic_missing_ids and \"longitude\" in dic_missing_ids[institution_name]:\n",
    "                                dic_papers[author_id][work_id][\"longitude\"] += [dic_missing_ids[institution_name][\"longitude\"]]\n",
    "                                dic_papers[author_id][work_id][\"latitude\"] += [dic_missing_ids[institution_name][\"latitude\"]]\n",
    "\n",
    "                            else:\n",
    "                                dic = get_location_missing( institution_name )\n",
    "                                dic_missing_ids[institution_name] = dic\n",
    "                                if \"longitude\" in dic:\n",
    "                                    dic_papers[author_id][work_id][\"longitude\"] += [dic[\"longitude\"]]\n",
    "                                    dic_papers[author_id][work_id][\"latitude\"] += [dic[\"latitude\"]]\n",
    "\n",
    "\n",
    "\n",
    "                        dic_encoding[author_id][work_id] = {}\n",
    "\n",
    "                        try:\n",
    "                            if line[1] != None:\n",
    "                                dic_encoding[author_id][work_id][\"encoded_title\"] = clean_encoding(line[1])\n",
    "                            else:\n",
    "                                dic_encoding[author_id][work_id][\"encoded_title\"] = None\n",
    "\n",
    "                        except:\n",
    "                            dic_encoding[author_id][work_id][\"encoded_title\"] = None\n",
    "                            print(author_id , work_id)\n",
    "                            pass\n",
    "                        \n",
    "                        try:\n",
    "                            if line[2] != None:\n",
    "                                dic_encoding[author_id][work_id][\"encoded_abstract\"] = clean_encoding(line[2])\n",
    "                            else:\n",
    "                                dic_encoding[author_id][work_id][\"encoded_abstract\"] = None\n",
    "\n",
    "                        except:\n",
    "                            dic_encoding[author_id][work_id][\"encoded_abstract\"] = None\n",
    "                            print(author_id , work_id)\n",
    "                            pass\n",
    "                        \n",
    "\n",
    "\n",
    "            good_similarity = 0        \n",
    "                     \n",
    "            for work_id in dic_encoding[author_id]:\n",
    "                \n",
    "                \n",
    "                paper = dic_encoding[author_id][work_id]\n",
    "                \n",
    "                for inventor_id in PatentsView_id:\n",
    "                \n",
    "                    for patent_id in dic_patents[inventor_id]:\n",
    "                        \n",
    "                        patent = dic_patents[inventor_id][patent_id]\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        sim_abstract = similarity_abstract(paper , patent)\n",
    "                        sim_title = similarity_title(paper , patent)\n",
    "                        \n",
    "                        if sim_abstract != None and sim_title != None:\n",
    "                            dic_comparison[author_id][work_id + \" \" + \"US-\" +patent_id] = {}\n",
    "                            dic_comparison[author_id][work_id + \" \" + \"US-\" +patent_id][\"similarity_abstract\"]= sim_abstract\n",
    "                            dic_comparison[author_id][work_id + \" \" + \"US-\" +patent_id][\"similarity_title\"]= sim_title\n",
    "                        \n",
    "                \n",
    "                \n",
    "    conn.close()\n",
    "                \n",
    "    \n",
    "    return  dic_papers , dic_patents , dic_comparison\n",
    "                    \n",
    "       \n",
    "        \n",
    "        \n",
    "#get the patents associated with the patent numbers stored in the dictionary dic_ids\n",
    "\n",
    "\n",
    "def get_papers2( dic_ids , dic_missing_ids , dic_papers):\n",
    "        \n",
    "    #establishing the connection\n",
    "    conn = psycopg2.connect(\"user=\" + user + \" password=\" + password)\n",
    "\n",
    "    #Creating a cursor object using the cursor() method\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "\n",
    "    for names in dic_ids:\n",
    "        \n",
    "        author_name = names.split(\"%\")[0]\n",
    "        set_author_ids = dic_ids[names][\"OpenAlex_id\"]\n",
    "        for author_id in set_author_ids:\n",
    "                        \n",
    "            if author_id in dic_papers and \"co-authors\" not in dic_papers[author_id]:\n",
    "                \n",
    "                for work in dic_papers[author_id]:\n",
    "\n",
    "                    text = \"\"\"SELECT  w.work_id , \n",
    "                                      w.publication_date , \n",
    "                                      w.concepts , \n",
    "                                      w.referenced_works,\n",
    "                                      string_agg( a.display_name , ';') \n",
    "                                      \n",
    "\n",
    "\n",
    "                              FROM works_OpenAlex AS w\n",
    "                              \n",
    "                              JOIN works_authors_OpenAlex AS wa ON wa.work_id =  w.work_id \n",
    "                              JOIN authors_OpenAlex AS a ON wa.author_id = a.author_id\n",
    "\n",
    "                              WHERE w.work_id = '\"\"\" + work + \"\"\"'\n",
    "                              GROUP BY w.work_id,\n",
    "                                       w.publication_date , \n",
    "                                       w.concepts , \n",
    "                                       w.referenced_works;\"\"\"\n",
    "\n",
    "                    cursor.execute(text)\n",
    "                    res = cursor.fetchall()\n",
    "                \n",
    "\n",
    "                    for line in res:\n",
    "                        \n",
    "\n",
    "                        work_id = line[0]\n",
    "                        dic_papers[author_id][work_id][\"co_authors\"] = [ normalize(elem) for elem in set(line[4].split(\";\")) ] \n",
    "\n",
    "\n",
    "\n",
    "                        if line[2] != None:\n",
    "                            dic_papers[author_id][work_id][\"concepts\"] = \", \".join(line[2].split(\"; \"))                        \n",
    "                        else:\n",
    "                            dic_papers[author_id][work_id][\"concepts\"] = None\n",
    "\n",
    "\n",
    "                        if line[3] != None:\n",
    "                            dic_papers[author_id][work_id][\"referenced_works\"] = set(line[3].split(\"; \"))\n",
    "                        else:\n",
    "                            dic_papers[author_id][work_id][\"referenced_works\"] = None\n",
    "\n",
    "\n",
    "                        if line[1] != None:\n",
    "                            dic_papers[author_id][work_id][\"publication_date\"] = line[1].year\n",
    "                        else:\n",
    "                            dic_papers[author_id][work_id][\"publication_date\"] = None\n",
    "\n",
    "\n",
    "                        dic_papers[author_id][work_id][\"author_name\"] = author_name\n",
    "                        \n",
    "                    \n",
    "            \n",
    "            \n",
    "            else:\n",
    "                print(author_id)\n",
    "\n",
    "                            \n",
    "    conn.close()\n",
    "    \n",
    "\n",
    "    return dic_papers\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b03761f-1ee7-45a2-93ef-38e8f3871740",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Compare papers and patents, and predict matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57eae2c4-b03c-4eef-8d70-08e0f581b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the features that are representing the similarity between a paper and a patent\n",
    "\n",
    "def get_dic_test(last_name):\n",
    "    \n",
    "    \n",
    "    #get the author ids, inventor ids, relative papers and patents, and similarity scores between the author and inventor names \n",
    "    dic_comparison , dic_ids , dic_authors = get_pd_table_comparison(last_name)[:3]\n",
    "    \n",
    "    dic_papers , dic_patents , dic_test_full =  save_time(dic_ids)\n",
    "    \n",
    "    dic_papers = get_papers2( dic_ids , dic_missing_ids , dic_papers)\n",
    "    \n",
    "    dic_test = {}\n",
    "    \n",
    "    for elem in dic_ids:\n",
    "        \n",
    "        author_name , inventor_name = elem.split(\"%\")\n",
    "\n",
    "        list_author_ids = dic_ids[elem][\"OpenAlex_id\"]\n",
    "\n",
    "        list_inventors_ids = dic_ids[elem][\"PatentsView_id\"]\n",
    "        \n",
    "        for author_id in list_author_ids:\n",
    "            \n",
    "            if author_id in dic_papers:\n",
    "            \n",
    "                dic_test = { **dic_test, **dic_test_full[author_id]}\n",
    "\n",
    "\n",
    "                data_author = dic_papers[author_id]\n",
    "                list_dates_author = [ data_author[paper][\"publication_date\"] for paper in data_author if \"publication_date\" in data_author[paper] and data_author[paper][\"publication_date\"] != None]\n",
    "                if list_dates_author != []:\n",
    "                    dic_dates = { \"list_dates\" : list_dates_author,\n",
    "                                     \"max_dates\" : max(list_dates_author),\n",
    "                                     \"min_dates\" : min(list_dates_author),\n",
    "                                     \"mean_dates\" : np.mean(list_dates_author),\n",
    "                                     \"chi_dist\" : chi2.fit(list_dates_author) } \n",
    "\n",
    "                else:\n",
    "                    dic_dates = None\n",
    "\n",
    "                data_author_ids = dic_authors[author_id]\n",
    "\n",
    "\n",
    "                for inventor_id in list_inventors_ids:\n",
    "\n",
    "                    data_inventor = dic_patents[inventor_id]\n",
    "\n",
    "                    for patent in data_inventor:\n",
    "\n",
    "                        data_patent = data_inventor[patent]\n",
    "\n",
    "                        for paper in data_author:\n",
    "\n",
    "                            data_paper = data_author[paper]\n",
    "\n",
    "                            if paper + \" \" + \"US-\" +patent in dic_test:\n",
    "                                \n",
    "\n",
    "                                if dic_test[paper + \" \" + \"US-\" +patent][\"similarity_title\"] == None or dic_test[paper + \" \" + \"US-\" +patent][\"similarity_abstract\"] == None:\n",
    "                                    dic_test.pop(paper + \" \" + \"US-\" +patent)\n",
    "                                \n",
    "                                else:\n",
    "                                    if \"co_authors\" in data_paper:\n",
    "                                        dic_test[paper + \" \" + \"US-\" +patent] = comparison_pairs2(dic_test[paper + \" \" + \"US-\" +patent] , data_paper , data_patent , data_author_ids , dic_dates)\n",
    "                                        dic_test[paper + \" \" + \"US-\" +patent][\"inventor_id\"] =  inventor_id\n",
    "                                        dic_test[paper + \" \" + \"US-\" +patent][\"author_id\"] = author_id\n",
    "                                        dic_test[paper + \" \" + \"US-\" +patent][\"author_name\"] = data_paper[\"author_name\"]\n",
    "                                        dic_test[paper + \" \" + \"US-\" +patent][\"inventor_name\"] = data_patent[\"inventor_name\"]\n",
    "                                        dic_test[paper + \" \" + \"US-\" +patent][\"name_score\"] = dic_comparison[data_paper[\"author_name\"]][data_patent[\"inventor_name\"]]\n",
    "\n",
    "                                    else:\n",
    "                                        dic_test.pop(paper + \" \" + \"US-\" +patent)\n",
    "\n",
    "                                \n",
    "                                \n",
    "                                #except:\n",
    "                                    #continue \n",
    "            \n",
    "    dic_test = { k : v for k,v in dic_test.items() if \"number_in_common_authors\" in v } \n",
    "\n",
    "    return dic_test\n",
    "                        \n",
    "                                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd5d0f9d-8027-4926-8312-01deec3fee2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity_abstract</th>\n",
       "      <th>similarity_title</th>\n",
       "      <th>number_in_common_authors</th>\n",
       "      <th>list_in_common_authors</th>\n",
       "      <th>distance_assignees</th>\n",
       "      <th>distance_inventors</th>\n",
       "      <th>in_common_citing_papers</th>\n",
       "      <th>similarity_institution</th>\n",
       "      <th>date_likelihood</th>\n",
       "      <th>date_difference</th>\n",
       "      <th>publications_range</th>\n",
       "      <th>number_publications</th>\n",
       "      <th>inventor_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>inventor_name</th>\n",
       "      <th>name_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>W4296951648 US-10167450</th>\n",
       "      <td>0.265928</td>\n",
       "      <td>0.079623</td>\n",
       "      <td>1</td>\n",
       "      <td>[r scharfmann-raphael scharfmann]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>fl:ra_ln:scharfmann-1</td>\n",
       "      <td>A5038394136</td>\n",
       "      <td>r scharfmann</td>\n",
       "      <td>raphael scharfmann</td>\n",
       "      <td>0.524048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4296951648 US-8679842</th>\n",
       "      <td>0.298008</td>\n",
       "      <td>0.083243</td>\n",
       "      <td>1</td>\n",
       "      <td>[r scharfmann-raphael scharfmann]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>fl:ra_ln:scharfmann-1</td>\n",
       "      <td>A5038394136</td>\n",
       "      <td>r scharfmann</td>\n",
       "      <td>raphael scharfmann</td>\n",
       "      <td>0.524048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W4296951648 US-9493743</th>\n",
       "      <td>0.285955</td>\n",
       "      <td>0.114219</td>\n",
       "      <td>1</td>\n",
       "      <td>[r scharfmann-raphael scharfmann]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>fl:ra_ln:scharfmann-1</td>\n",
       "      <td>A5038394136</td>\n",
       "      <td>r scharfmann</td>\n",
       "      <td>raphael scharfmann</td>\n",
       "      <td>0.524048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W138415571 US-10167450</th>\n",
       "      <td>0.2795</td>\n",
       "      <td>0.398843</td>\n",
       "      <td>2</td>\n",
       "      <td>[paul czernichow-paul czernichow, r scharfmann...</td>\n",
       "      <td>3.251521</td>\n",
       "      <td>3.251521</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.641087</td>\n",
       "      <td>9.769231</td>\n",
       "      <td>33</td>\n",
       "      <td>26</td>\n",
       "      <td>fl:ra_ln:scharfmann-1</td>\n",
       "      <td>A5053388694</td>\n",
       "      <td>r scharfmann</td>\n",
       "      <td>raphael scharfmann</td>\n",
       "      <td>0.524048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W138415571 US-8679842</th>\n",
       "      <td>0.349155</td>\n",
       "      <td>0.413615</td>\n",
       "      <td>1</td>\n",
       "      <td>[r scharfmann-raphael scharfmann]</td>\n",
       "      <td>3.251521</td>\n",
       "      <td>3.251521</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.425919</td>\n",
       "      <td>4.769231</td>\n",
       "      <td>33</td>\n",
       "      <td>26</td>\n",
       "      <td>fl:ra_ln:scharfmann-1</td>\n",
       "      <td>A5053388694</td>\n",
       "      <td>r scharfmann</td>\n",
       "      <td>raphael scharfmann</td>\n",
       "      <td>0.524048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        similarity_abstract similarity_title  \\\n",
       "W4296951648 US-10167450            0.265928         0.079623   \n",
       "W4296951648 US-8679842             0.298008         0.083243   \n",
       "W4296951648 US-9493743             0.285955         0.114219   \n",
       "W138415571 US-10167450               0.2795         0.398843   \n",
       "W138415571 US-8679842              0.349155         0.413615   \n",
       "\n",
       "                        number_in_common_authors  \\\n",
       "W4296951648 US-10167450                        1   \n",
       "W4296951648 US-8679842                         1   \n",
       "W4296951648 US-9493743                         1   \n",
       "W138415571 US-10167450                         2   \n",
       "W138415571 US-8679842                          1   \n",
       "\n",
       "                                                    list_in_common_authors  \\\n",
       "W4296951648 US-10167450                  [r scharfmann-raphael scharfmann]   \n",
       "W4296951648 US-8679842                   [r scharfmann-raphael scharfmann]   \n",
       "W4296951648 US-9493743                   [r scharfmann-raphael scharfmann]   \n",
       "W138415571 US-10167450   [paul czernichow-paul czernichow, r scharfmann...   \n",
       "W138415571 US-8679842                    [r scharfmann-raphael scharfmann]   \n",
       "\n",
       "                        distance_assignees distance_inventors  \\\n",
       "W4296951648 US-10167450               None               None   \n",
       "W4296951648 US-8679842                None               None   \n",
       "W4296951648 US-9493743                None               None   \n",
       "W138415571 US-10167450            3.251521           3.251521   \n",
       "W138415571 US-8679842             3.251521           3.251521   \n",
       "\n",
       "                        in_common_citing_papers similarity_institution  \\\n",
       "W4296951648 US-10167450                       0                      0   \n",
       "W4296951648 US-8679842                        0                      0   \n",
       "W4296951648 US-9493743                        0                      0   \n",
       "W138415571 US-10167450                        0                      1   \n",
       "W138415571 US-8679842                         0                      0   \n",
       "\n",
       "                        date_likelihood date_difference publications_range  \\\n",
       "W4296951648 US-10167450           -5000             3.0                  0   \n",
       "W4296951648 US-8679842            -5000             8.0                  0   \n",
       "W4296951648 US-9493743            -5000             6.0                  0   \n",
       "W138415571 US-10167450        -4.641087        9.769231                 33   \n",
       "W138415571 US-8679842         -4.425919        4.769231                 33   \n",
       "\n",
       "                        number_publications            inventor_id  \\\n",
       "W4296951648 US-10167450                   1  fl:ra_ln:scharfmann-1   \n",
       "W4296951648 US-8679842                    1  fl:ra_ln:scharfmann-1   \n",
       "W4296951648 US-9493743                    1  fl:ra_ln:scharfmann-1   \n",
       "W138415571 US-10167450                   26  fl:ra_ln:scharfmann-1   \n",
       "W138415571 US-8679842                    26  fl:ra_ln:scharfmann-1   \n",
       "\n",
       "                           author_id   author_name       inventor_name  \\\n",
       "W4296951648 US-10167450  A5038394136  r scharfmann  raphael scharfmann   \n",
       "W4296951648 US-8679842   A5038394136  r scharfmann  raphael scharfmann   \n",
       "W4296951648 US-9493743   A5038394136  r scharfmann  raphael scharfmann   \n",
       "W138415571 US-10167450   A5053388694  r scharfmann  raphael scharfmann   \n",
       "W138415571 US-8679842    A5053388694  r scharfmann  raphael scharfmann   \n",
       "\n",
       "                        name_score  \n",
       "W4296951648 US-10167450   0.524048  \n",
       "W4296951648 US-8679842    0.524048  \n",
       "W4296951648 US-9493743    0.524048  \n",
       "W138415571 US-10167450    0.524048  \n",
       "W138415571 US-8679842     0.524048  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = get_dic_test('scharfmann')\n",
    "table = pd.DataFrame(dic).T\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2bf5e72-2701-4d8d-b461-ca7ac41d80ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle \n",
    "\n",
    "#load the model from disk\n",
    "rf_filename_distance = main_path + 'train_the_model/random_forest_distance.sav'\n",
    "random_forest_distance = pickle.load(open(rf_filename_distance, 'rb'))\n",
    "\n",
    "rf_filename_no_distance = main_path + 'train_the_model/random_forest_no_distance.sav'\n",
    "random_forest_no_distance = pickle.load(open(rf_filename_no_distance, 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prediction(name):\n",
    "    \n",
    "    dic_res = get_dic_test(name)\n",
    "    \n",
    "    dic_pred_dist = { ids : { k : dic_res[ids][k] for k in ['number_in_common_authors', 'similarity_title', 'similarity_abstract', 'distance_assignees', 'distance_inventors', 'in_common_citing_papers','similarity_institution', 'date_likelihood', 'date_difference']} for ids in dic_res if pd.isna(dic_res[ids][\"distance_assignees\"]) == False and  pd.isna(dic_res[ids][\"distance_inventors\"]) == False }\n",
    "    dic_pred_no_dist = { ids : { k : dic_res[ids][k] for k in ['number_in_common_authors', 'similarity_title', 'similarity_abstract',  'in_common_citing_papers','similarity_institution', 'date_likelihood', 'date_difference']} for ids in dic_res if pd.isna(dic_res[ids][\"distance_assignees\"]) == True or  pd.isna(dic_res[ids][\"distance_inventors\"]) == True }\n",
    "\n",
    "    res_dist = np.array([ list(item.values()) for item in list(dic_pred_dist.values()) ])\n",
    "    res_no_dist = np.array([ list(item.values()) for item in list(dic_pred_no_dist.values()) ])\n",
    "    \n",
    "    if len(res_no_dist) > 0:\n",
    "        no_dist_pred = random_forest_no_distance.predict_proba(res_no_dist)[:,1]\n",
    "    else:\n",
    "        no_dist_pred = []\n",
    "    \n",
    "    if len(res_dist) >0:\n",
    "        dist_pred = random_forest_distance.predict_proba(res_dist)[:,1]\n",
    "    else:\n",
    "        dist_pred = []\n",
    "        \n",
    "\n",
    "    keys = list(dic_pred_dist.keys())\n",
    "    for k in range(len(keys)):\n",
    "        dic_res[keys[k]][\"prediction\"] = dist_pred[k]\n",
    "\n",
    "    keys = list(dic_pred_no_dist.keys())\n",
    "    for k in range(len(keys)):\n",
    "        dic_res[keys[k]][\"prediction\"] = no_dist_pred[k]\n",
    "\n",
    "    dic_results = { k : v for k,v in dic_res.items() if v[\"prediction\"] + v[\"name_score\"]**2 > 1.25}\n",
    "    results = np.array([ [ v[\"author_id\"], v[\"inventor_id\"] ] for v in dic_results.values() ])\n",
    "    \n",
    "\n",
    "    dic_final = {}\n",
    "    \n",
    "    \n",
    "    for line in dic_results:\n",
    "        author_id = dic_results[line][\"author_id\"]\n",
    "        inventor_id = dic_results[line][\"inventor_id\"]\n",
    "        if inventor_id + \" \" + author_id not in dic_final:\n",
    "            dic_final[inventor_id + \" \" + author_id] = {}\n",
    "            dic_final[inventor_id + \" \" + author_id]['inventor_id'] = inventor_id\n",
    "            dic_final[inventor_id + \" \" + author_id]['author_id'] = author_id\n",
    "            dic_final[inventor_id + \" \" + author_id]['inventor_name'] = dic_results[line][\"inventor_name\"]\n",
    "            dic_final[inventor_id + \" \" + author_id]['author_name'] = dic_results[line][\"author_name\"]\n",
    "            dic_final[inventor_id + \" \" + author_id]['number of match'] = 0\n",
    "            dic_final[inventor_id + \" \" + author_id][\"name score\"] = dic_results[line][\"name_score\"]\n",
    "        dic_final[inventor_id + \" \" + author_id]['number of match'] += 1\n",
    "        \n",
    "            \n",
    "            \n",
    "    for line in dic_final:\n",
    "        author_id = dic_final[line][\"author_id\"]\n",
    "        inventor_id = dic_final[line][\"inventor_id\"]\n",
    "        count = len({ k:v for k,v in dic_res.items() if v[\"author_id\"] == author_id and v[\"inventor_id\"] == inventor_id})\n",
    "        dic_final[line][\"number of comparison\"] = count\n",
    "\n",
    "\n",
    "    df_final = pd.DataFrame(dic_final , index = [ 'inventor_id' , \"author_id\", \"number of comparison\", \"number of match\", \"author_name\" , \"inventor_name\" , \"name score\"]).T    \n",
    "\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d838252c-82ab-462d-a2a4-3378ed47a0fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Try the identification process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "238b6918-5660-40b5-8b29-332efcfd2ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"scharfmann\"\n",
    "df_test = prediction(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27745ea3-3882-4d9b-b2ac-15335a594e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inventor_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>number of comparison</th>\n",
       "      <th>number of match</th>\n",
       "      <th>author_name</th>\n",
       "      <th>inventor_name</th>\n",
       "      <th>name score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fl:ra_ln:scharfmann-1 A5053388694</th>\n",
       "      <td>fl:ra_ln:scharfmann-1</td>\n",
       "      <td>A5053388694</td>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>r scharfmann</td>\n",
       "      <td>raphael scharfmann</td>\n",
       "      <td>0.524048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fl:ra_ln:scharfmann-1 A5001408571</th>\n",
       "      <td>fl:ra_ln:scharfmann-1</td>\n",
       "      <td>A5001408571</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>raphael scharfmann</td>\n",
       "      <td>raphael scharfmann</td>\n",
       "      <td>0.732212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fl:ra_ln:scharfmann-1 A5007387317</th>\n",
       "      <td>fl:ra_ln:scharfmann-1</td>\n",
       "      <td>A5007387317</td>\n",
       "      <td>558</td>\n",
       "      <td>555</td>\n",
       "      <td>raphael scharfmann</td>\n",
       "      <td>raphael scharfmann</td>\n",
       "      <td>0.732212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             inventor_id    author_id  \\\n",
       "fl:ra_ln:scharfmann-1 A5053388694  fl:ra_ln:scharfmann-1  A5053388694   \n",
       "fl:ra_ln:scharfmann-1 A5001408571  fl:ra_ln:scharfmann-1  A5001408571   \n",
       "fl:ra_ln:scharfmann-1 A5007387317  fl:ra_ln:scharfmann-1  A5007387317   \n",
       "\n",
       "                                  number of comparison number of match  \\\n",
       "fl:ra_ln:scharfmann-1 A5053388694                   78              78   \n",
       "fl:ra_ln:scharfmann-1 A5001408571                   21              21   \n",
       "fl:ra_ln:scharfmann-1 A5007387317                  558             555   \n",
       "\n",
       "                                          author_name       inventor_name  \\\n",
       "fl:ra_ln:scharfmann-1 A5053388694        r scharfmann  raphael scharfmann   \n",
       "fl:ra_ln:scharfmann-1 A5001408571  raphael scharfmann  raphael scharfmann   \n",
       "fl:ra_ln:scharfmann-1 A5007387317  raphael scharfmann  raphael scharfmann   \n",
       "\n",
       "                                  name score  \n",
       "fl:ra_ln:scharfmann-1 A5053388694   0.524048  \n",
       "fl:ra_ln:scharfmann-1 A5001408571   0.732212  \n",
       "fl:ra_ln:scharfmann-1 A5007387317   0.732212  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b026c4de-b28a-4f81-932b-37557a0743b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"doudna\"\n",
    "df_test = prediction(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bba0ca8-89e6-47f5-bca3-7e9a9fd2d76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inventor_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>number of comparison</th>\n",
       "      <th>number of match</th>\n",
       "      <th>author_name</th>\n",
       "      <th>inventor_name</th>\n",
       "      <th>name score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fl:je_ln:doudna-1 A5067184382</th>\n",
       "      <td>fl:je_ln:doudna-1</td>\n",
       "      <td>A5067184382</td>\n",
       "      <td>48471</td>\n",
       "      <td>48340</td>\n",
       "      <td>jennifer a doudna</td>\n",
       "      <td>jennifer a doudna</td>\n",
       "      <td>0.787591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     inventor_id    author_id  \\\n",
       "fl:je_ln:doudna-1 A5067184382  fl:je_ln:doudna-1  A5067184382   \n",
       "\n",
       "                              number of comparison number of match  \\\n",
       "fl:je_ln:doudna-1 A5067184382                48471           48340   \n",
       "\n",
       "                                     author_name      inventor_name name score  \n",
       "fl:je_ln:doudna-1 A5067184382  jennifer a doudna  jennifer a doudna   0.787591  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b538b7-305f-42a1-82e6-30682c87dbae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Run the entire process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88773fb9-774b-488f-86c8-d54da2e66d51",
   "metadata": {},
   "source": [
    "Use the two python files \"gatekeepers_uncommon_names.py\" and \"gatekeepers_common_names.py\" to run the process on all PatentsView last names. \n",
    "For very common last names, the RAM memory tends to blow up. Therefore, two python files are provided. \n",
    "- Start with \"gatekeepers_uncommon_names.py\", which run the process on all last names, expect the 8,000 most common last names. The parallelisation is coded such that each CPU run one last name. \n",
    "- Then, use \"gatekeepers_common_names.py\" to run the remaining last names. \"gatekeepers_common_names.py\". The parallelisation is coded such that the last names go into the process one by one. Then, each CPU run one first name. It allows to cut into piece very common last names and avoid to excede the RAM memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c283f4-e11f-4b9c-b3ca-e601888b9068",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Clean the SI dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2477abc5-01da-4459-9f76-2c28503bcd87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Merge SI files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824a621f-9155-489f-a074-0c414e729654",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.concat( [ pd.read_csv(file , delimiter = \"\\t\" , names = ['inventor_id', 'author_id',\n",
    "        'number of comparison','number of match', 'inventor_name', 'author_name','name score']) for file in glob.glob(main_path + \"run_SI_dataset/Results/*\") ])\n",
    "\n",
    "table = table.drop_duplicates()\n",
    "table.to_csv(main_path + \"gatekeepers_intermediate_v2.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fd7dae-9c5c-4185-8d1b-b3880980b266",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Remove inconsistency "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3f3d5b-2eee-4e72-9cc2-a50a1f75a1b7",
   "metadata": {},
   "source": [
    "The high level idea of this section is to remove the inconsistency in the SI dataset. If J. Doudna is linked with John Doudna and J. Doudna is linked with Jennifer Doudna, it is an inconsistency. Removing all SIs associated with inconsistencies is very conservative. Instead, we identify inconsistencies, and then remove the low condifidence links. Therefore, the inconsistency are removed and we keep the high confidence SIs. We processed in two rounds: the first round identifies and flags the inconsistencies, and remove the inconsistent data with a confidence < 0.5. The second round identifies and flags the remaining inconsistencies, and remove the inconsistent data with a confidence < 0.9 and with more than 100 comparisons. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a1178-0b51-41da-baf3-d1e3bc6f5d94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load data and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b752678f-7ac1-47fc-b35b-483ede3602df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load function to clean the names \n",
    "\n",
    "##name cleaning - elements to remove or merge from the names \n",
    "name_del = [\"2nd\", \"3rd\", \"jr\", \"jr.\", \"junior\", \"sr\", \"sr.\", \"senior\", \"i\", 'ii' , 'iii']\n",
    "\n",
    "ln_suff= [\"oster\", \"nordre\", \"vaster\", \"aust\", \"vesle\", \"da\", \"van t\", \"af\", \"al\", \"setya\", \"zu\", \"la\", \"na\", \"mic\", \"ofver\", \"el\", \"vetle\", \"van het\", \"dos\", \"ui\", \"vest\", \"ab\", \"vste\", \"nord\", \"van der\", \"bin\", \"ibn\", \"war\", \"fitz\", \"alam\", \"di\", \"erch\", \"fetch\", \"nga\", \"ka\", \"soder\", \"lille\", \"upp\", \"ua\", \"te\", \"ni\", \"bint\", \"von und zu\", \"vast\", \"vestre\", \"over\", \"syd\", \"mac\", \"nin\", \"nic\", \"putri\", \"bet\", \"verch\", \"norr\", \"bath\", \"della\", \"van\", \"ben\", \"du\", \"stor\", \"das\", \"neder\", \"abu\", \"degli\", \"vre\", \"ait\", \"ny\", \"opp\", \"pour\", \"kil\", \"der\", \"oz\",  \"von\", \"at\", \"nedre\", \"van den\", \"setia\", \"ap\", \"gil\", \"myljom\", \"van de\", \"stre\", \"dele\", \"mck\", \"de\", \"mellom\", \"mhic\", \"binti\", \"ath\", \"binte\", \"snder\", \"sre\", \"ned\", \"ter\", \"bar\", \"le\", \"mala\", \"ost\", \"syndre\", \"sr\", \"bat\", \"sndre\", \"austre\", \"putra\", \"putera\", \"av\", \"lu\", \"vetch\", \"ver\", \"puteri\", \"mc\", \"tre\", \"st\"]\n",
    "\n",
    "\n",
    "## load dictionary with the distribution of the last names: \n",
    "f = open(main_path + \"frequency_last_names.json\",\"r\")\n",
    "import json\n",
    "dic_last_names = json.load(f)\n",
    "\n",
    "## load dictionary with the distribution of the first names: \n",
    "f = open(main_path + \"frequency_first_names.json\",\"r\")\n",
    "import json\n",
    "dic_first_names = json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "########################################### Fonctions to load  ###############################################################\n",
    "\n",
    "\n",
    "#merge the particles/suffixes/prefixes with the last name \n",
    "#ln_suff file can be modified if more or less suffixes want to be merged \n",
    "def ln_suff_merge(string):\n",
    "    for suff in ln_suff:\n",
    "        if f\"{' ' + suff + ' '}\" in string or string.startswith(f\"{suff + ' '}\"):\n",
    "            string =  string.replace(f\"{suff + ' '}\", suff.replace(\" \",\"\"))\n",
    "    return string\n",
    "\n",
    "\n",
    "#suppress all the unwanted suffixes from a string\n",
    "#name_del file can be modified if more or less suffixes want to be suppressed \n",
    "def name_delete(string):\n",
    "    for suff in name_del:\n",
    "        if f\"{' ' + suff + ' '}\" in string or string.endswith(f\"{' ' + suff}\"):\n",
    "            string =  string.replace(f\"{suff}\",\"\")\n",
    "    return string\n",
    "\n",
    "\n",
    "#normalize a string dat that represents often a name. \n",
    "def normalize(data):\n",
    "    normal = unicodedata.normalize('NFKD', data).encode('ASCII', 'ignore')\n",
    "    val = normal.decode(\"utf-8\")\n",
    "    # delete unwanted elmt\n",
    "    val = name_delete(val)\n",
    "    # lower full name in upper\n",
    "    val = re.sub(r\"[A-Z]{3,}\", lambda x: x.group().lower(), val)\n",
    "    # add space in front of upper case\n",
    "    if \"Mac\" not in val and \"Mc\" not in val:\n",
    "        val = re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", val)\n",
    "    # Lower case\n",
    "    val = val.lower()\n",
    "    # remove special characters\n",
    "    val = re.sub('[^A-Za-z0-9 -]+', ' ', val)\n",
    "    # remove multiple spaces\n",
    "    val = re.sub(' +', ' ', val)\n",
    "    # remove trailing spaces\n",
    "    val = val.strip()\n",
    "    # suffix merge\n",
    "    val = ln_suff_merge(val)\n",
    "\n",
    "    return val\n",
    "\n",
    "\n",
    "#normalize a string dat that represents an institution. \n",
    "def normalize_inst(data):\n",
    "\n",
    "    # Lower case\n",
    "    data = data.lower()\n",
    "    # remove special characters\n",
    "    data = re.sub('[^A-Za-z0-9 ]+', ' ', data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "#return a ratio of similarity of letters between two strings (to handle in the first names errors)\n",
    "def match_ratio(string1,string2):\n",
    "    return fuzz.ratio(string1, string2)\n",
    "\n",
    "\n",
    "#return 4 if string1 and string2 are the same\n",
    "#return 3 if string1 and string2 sound the same\n",
    "#otherwise, return less\n",
    "def metaphone(string1,string2):\n",
    "    if string1==string2:\n",
    "        return 4\n",
    "    tuple1 = doublemetaphone(string1)\n",
    "    tuple2 = doublemetaphone(string2)\n",
    "    if tuple1[0] == tuple2[0]:\n",
    "        return 3\n",
    "    elif tuple1[0] == tuple2[1] or tuple1[1] == tuple2[0]:\n",
    "        return 2\n",
    "    elif tuple1[1] == tuple2[1]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "#compare name1 and name2. Return 1 if name1 and name2 might represent the same individual, otherwise 0.\n",
    "def comparison(name1 , name2):\n",
    "    \n",
    "    if name1 == name2:\n",
    "        return 1\n",
    "    \n",
    "    #if there is no first name, retrun it's a match\n",
    "    if name1 == \"\" or name2 == \"\":\n",
    "        return 1\n",
    "    \n",
    "    #if some first names exist:\n",
    "    list_name1 = name1.split()\n",
    "    list_name2 = name2.split()\n",
    "    \n",
    "    #minimum number of first names to match\n",
    "    number_match = min( len(list_name1) , len(list_name2))\n",
    "    \n",
    "    #for each name, check if there is a match\n",
    "    count_match = 0\n",
    "    for elem1 in list_name1:\n",
    "        match = 0\n",
    "        \n",
    "        #if we just have the initial:\n",
    "        if len(elem1) == 1:\n",
    "            for elem2 in list_name2:\n",
    "                if elem1[0] == elem2[0]:\n",
    "                    match = 1\n",
    "        else:\n",
    "            for elem2 in list_name2:\n",
    "                #if we just have the initial:\n",
    "                if len(elem2) == 1 and elem1[0] == elem2[0]:\n",
    "                    match = 1\n",
    "                    \n",
    "                #if elem1 and elem2 are entire first names that sound the same and have a ratio of common letters higher thsan 85%, it's a match\n",
    "                elif len(elem2) > 1 and (metaphone(elem1,elem2) > 2 or match_ratio(elem1 , elem2) > 85 ) :\n",
    "                    match = 1\n",
    "                    \n",
    "        #count the number of first names that match    \n",
    "        count_match += match\n",
    "        \n",
    "    #check if we have enough first names that match \n",
    "    if count_match < number_match:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4876de18-e741-416e-a99d-e247e5f58429",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load SI data \n",
    "\n",
    "table = pd.read_csv(main_path + \"gatekeepers_intermediate_v2.tsv\" , delimiter = \"\\t\", names = ['inventor_id', 'author_id', 'number of comparison', 'number of match',\n",
    "        'author_name', 'inventor_name', 'name score' ],  header = 0)\n",
    "df = table.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88174e5f-1959-4555-a538-f14b5c49a0d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### First round of cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff8856-03b9-4781-ad4a-61bb0be5da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## group SIs by transitivity\n",
    "\n",
    "data_test = df[[\"inventor_id\" , \"author_id\"]].to_numpy()\n",
    "\n",
    "cluster_test = []\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(data_test)\n",
    "count = 0 \n",
    "for connected_component in nx.connected_components(G):\n",
    "    cluster_test.append(connected_component)\n",
    "    \n",
    "dic_inventor_name = df[[\"inventor_id\" , \"inventor_name\"]].drop_duplicates(\"inventor_id\").set_index(\"inventor_id\").to_dict(\"index\")\n",
    "dic_author_name = df[[\"author_id\" , \"author_name\"]].drop_duplicates(\"author_id\").set_index(\"author_id\").to_dict(\"index\")\n",
    "\n",
    "dic_clusters = {}\n",
    "count = 0 \n",
    "\n",
    "for cluster in tqdm(cluster_test):\n",
    "    dic_clusters[count] = {}\n",
    "    dic_clusters[count][\"OA\"] = []\n",
    "    dic_clusters[count][\"PV\"] = []\n",
    "    for elem in cluster:\n",
    "        \n",
    "        if elem[0] == \"A\":\n",
    "            dic_clusters[count][\"OA\"].append(  ( elem , dic_author_name[elem]) )\n",
    "        else:\n",
    "            dic_clusters[count][\"PV\"].append( ( elem ,  dic_inventor_name[elem]))\n",
    "        \n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fcc1de-9d10-4fd3-b5e6-2b6a9e787a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors(i):\n",
    "    \n",
    "     \"\"\"\n",
    "    This function identifies inventor IDs and author IDS associated with potential errors in SI linkage. \n",
    "\n",
    "    Parameters:\n",
    "    i (int): The starting index for the clustering dictionary (for parallelization).\n",
    "\n",
    "    Returns:\n",
    "    inventor_ids (set): A set of inventor IDs associated with potential errors in the SI linkage. \n",
    "\n",
    "    Note:\n",
    "    - The function assumes that the `dic_clusters` dictionary and the `comparison` function are defined elsewhere in the code.\n",
    "    - The function iterates over the clustering dictionary in batches defined by the `workers` variable and checks for clusters with more than one unique inventor name.\n",
    "    - The function uses the `comparison` function to compare inventor names within each cluster. If any two names are found to be identical, the function adds the associated inventor IDs to the `inventor_ids` set.\n",
    "    - The function returns the `inventor_ids` set containing all inventor IDs associated with potential errors in inventor name clustering.\n",
    "    \"\"\"\n",
    "    \n",
    "    inventor_ids = set()\n",
    "    workers = 24\n",
    "    list_keys = list(dic_clusters.keys())\n",
    "    list_index = [ k for k in range(i,len(list_keys), workers)] \n",
    "    \n",
    "    for k in list_index:\n",
    "        \n",
    "        count = list_keys[k]\n",
    "        \n",
    "        if len(dic_clusters[count][\"PV\"]) > 1:\n",
    "            list_names = list(set([ elem[1][\"inventor_name\"] for elem in dic_clusters[count][\"PV\"] ])) \n",
    "            check = 0 \n",
    "            for k in range(len(list_names)):\n",
    "                for j in range(len(list_names)):\n",
    "                    comp = comparison(list_names[k] , list_names[j])\n",
    "                    if comp == 0:\n",
    "                        check = 1\n",
    "                        break\n",
    "            if check == 1:\n",
    "                inventor_ids = inventor_ids.union(set([ elem[0] for elem in dic_clusters[count][\"PV\"] ] ))\n",
    "                \n",
    "        if count in dic_clusters and len(dic_clusters[count][\"OA\"]) > 1:\n",
    "            list_names = list(set([ elem[1][\"author_name\"] for elem in dic_clusters[count][\"OA\"] ] ))\n",
    "            check = 0 \n",
    "            for k in range(len(list_names)):\n",
    "                for j in range(len(list_names)):\n",
    "                    comp = comparison(list_names[k] , list_names[j])\n",
    "                    if comp == 0:\n",
    "                        check = 1\n",
    "                        break\n",
    "            if check == 1:\n",
    "                inventor_ids = inventor_ids.union(set([ elem[0] for elem in dic_clusters[count][\"PV\"] ] ))\n",
    "\n",
    "    return inventor_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1e64de-6c23-4089-8bb1-eb727865f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run function using 24 cpus \n",
    "\n",
    "workers = 24\n",
    "p = Pool(workers)\n",
    "func = partial(get_errors)\n",
    "errors_inventors_ids = p.map(func, [ i  for i in range(workers)])\n",
    "p.close()\n",
    "\n",
    "## get inventor ids associated with potential linkage errors \n",
    "\n",
    "inventor_ids = []\n",
    "for elem in errors_inventors_ids:\n",
    "    inventor_ids += list(elem)\n",
    "inventor_ids = set(inventor_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d55c5-37ff-4da0-9819-1c924be0b6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## flag the potential errors \n",
    "\n",
    "inventors = df[\"inventor_id\"].tolist()\n",
    "df[\"flag\"] = [ 1 if inventor_id in inventor_ids else 0 for inventor_id in inventors]\n",
    "\n",
    "## keep unflagged data OR high confidence data \n",
    "\n",
    "df = df[(df[\"flag\"] == 0) | ( (df[\"flag\"] == 1)  & (df[\"number of match\"]/df[\"number of comparison\"] > 0.5)) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e68c73-daa8-47b9-8f08-654a6a351a65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Second round of cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc5198-3f1a-480c-aea3-afc29dfc4628",
   "metadata": {},
   "outputs": [],
   "source": [
    "## group SIs by transitivity\n",
    "\n",
    "\n",
    "data_test = df[[\"inventor_id\" , \"author_id\"]].to_numpy()\n",
    "\n",
    "cluster_test = []\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(data_test)\n",
    "count = 0 \n",
    "for connected_component in nx.connected_components(G):\n",
    "    cluster_test.append(connected_component)\n",
    "    \n",
    "\n",
    "dic_inventor_name = df[[\"inventor_id\" , \"inventor_name\"]].drop_duplicates(\"inventor_id\").set_index(\"inventor_id\").to_dict(\"index\")\n",
    "dic_author_name = df[[\"author_id\" , \"author_name\"]].drop_duplicates(\"author_id\").set_index(\"author_id\").to_dict(\"index\")\n",
    "\n",
    "dic_clusters = {}\n",
    "count = 0 \n",
    "\n",
    "for cluster in tqdm(cluster_test):\n",
    "    dic_clusters[count] = {}\n",
    "    dic_clusters[count][\"OA\"] = []\n",
    "    dic_clusters[count][\"PV\"] = []\n",
    "    for elem in cluster:\n",
    "        \n",
    "        if elem[0] == \"A\":\n",
    "            dic_clusters[count][\"OA\"].append(  ( elem , dic_author_name[elem]) )\n",
    "        else:\n",
    "            dic_clusters[count][\"PV\"].append( ( elem ,  dic_inventor_name[elem]))\n",
    "        \n",
    "    count += 1\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1fa62-5760-4216-ba11-233220baf176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors2(i):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function identifies inventor IDs and author IDS associated with potential errors in SI linkage. \n",
    "\n",
    "    Parameters:\n",
    "    i (int): The starting index for the clustering dictionary (for parallelization).\n",
    "\n",
    "    Returns:\n",
    "    inventor_ids (set): A set of inventor IDs associated with potential errors in the SI linkage. \n",
    "\n",
    "    Note:\n",
    "    - The function assumes that the `dic_clusters` dictionary and the `comparison` function are defined elsewhere in the code.\n",
    "    - The function iterates over the clustering dictionary in batches defined by the `workers` variable and checks for clusters with more than one unique inventor name.\n",
    "    - The function uses the `comparison` function to compare inventor names within each cluster. If any two names are found to be identical, the function adds the associated inventor IDs to the `inventor_ids` set.\n",
    "    - The function returns the `inventor_ids` set containing all inventor IDs associated with potential errors in inventor name clustering.\n",
    "    \"\"\"\n",
    "    \n",
    "    inventor_ids = set()\n",
    "    workers = 24\n",
    "    list_keys = list(dic_clusters.keys())\n",
    "    list_index = [ k for k in range(i,len(list_keys), workers)] \n",
    "\n",
    "    for k in list_index:\n",
    "        \n",
    "        count = list_keys[k]        \n",
    "        if len(dic_clusters[count][\"PV\"]) > 1:\n",
    "            list_names = list(set([ elem[1][\"inventor_name\"] for elem in dic_clusters[count][\"PV\"] ])) \n",
    "            check = 0 \n",
    "            for k in range(len(list_names)):\n",
    "                for j in range(len(list_names)):\n",
    "                    comp = comparison(list_names[k] , list_names[j])\n",
    "                    if comp == 0:\n",
    "                        check = 1\n",
    "                        break\n",
    "            if check == 1:\n",
    "                inventor_ids = inventor_ids.union(set([ elem[0] for elem in dic_clusters[count][\"PV\"] ] ))\n",
    "                \n",
    "        if count in dic_clusters and len(dic_clusters[count][\"OA\"]) > 1:\n",
    "            list_names = list(set([ elem[1][\"author_name\"] for elem in dic_clusters[count][\"OA\"] ] ))\n",
    "            check = 0 \n",
    "            for k in range(len(list_names)):\n",
    "                for j in range(len(list_names)):\n",
    "                    comp = comparison(list_names[k] , list_names[j])\n",
    "                    if comp == 0:\n",
    "                        check = 1\n",
    "                        break\n",
    "            if check == 1:\n",
    "                inventor_ids = inventor_ids.union(set([ elem[0] for elem in dic_clusters[count][\"PV\"] ] ))\n",
    "\n",
    "    return inventor_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fcf2f9-9094-4df7-985e-af8d47113fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run function using 24 cpus \n",
    "\n",
    "workers = 24\n",
    "p = Pool(workers)\n",
    "func = partial(get_errors2)\n",
    "errors_inventors_ids2 = p.map(func, [ i  for i in range(workers)])\n",
    "p.close()\n",
    "\n",
    "## get inventor ids associated with potential linkage errors \n",
    "\n",
    "inventor_ids2 = []\n",
    "for elem in errors_inventors_ids2:\n",
    "    inventor_ids2 += list(elem)\n",
    "inventor_ids2 = set(inventor_ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7ff3b0-e80f-4d2c-845f-1391a2b53d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## flag the potential errors \n",
    "\n",
    "inventors = df[\"inventor_id\"].tolist()\n",
    "df[\"flag\"] = [ 1 if inventor_id in inventor_ids2 else 0 for inventor_id in inventors]\n",
    "\n",
    "## keep unflagged data OR very high confidence data \n",
    "\n",
    "df = df[(df[\"flag\"] == 0) | ( (df[\"flag\"] == 1) & (df[\"number of match\"] >= 100) & (df[\"number of match\"]/df[\"number of comparison\"] > 0.9)) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7531bc0d-a030-4881-9f98-a4644a648560",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Save the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207362b5-43bd-4d96-8d1a-1c778e60769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(['inventor_id','author_id'])\n",
    "df.to_csv(main_path + \"gatekeepers_intermediate_clean_v5.tsv\" , sep = \"\\t\" , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb0a24f-4f07-4fef-9fcd-520194b84d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## count number of SIs\n",
    "\n",
    "data_test = df[[\"inventor_id\" , \"author_id\"]].to_numpy()\n",
    "\n",
    "cluster_test = []\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(data_test)\n",
    "count = 0 \n",
    "for connected_component in nx.connected_components(G):\n",
    "    cluster_test.append(connected_component)\n",
    "    \n",
    "print(len(cluster_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
